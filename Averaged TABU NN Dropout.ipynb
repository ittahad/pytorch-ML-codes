{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3751,
     "status": "ok",
     "timestamp": 1555138488774,
     "user": {
      "displayName": "Ittahaduzzaman Akash",
      "photoUrl": "https://lh3.googleusercontent.com/-YERuM7ONE4k/AAAAAAAAAAI/AAAAAAAAAM8/wlRqU6U6u18/s64/photo.jpg",
      "userId": "11972750857132575782"
     },
     "user_tz": -360
    },
    "id": "FeFfkAp7HGSl",
    "outputId": "1c6e83db-af88-44a5-ac9a-cf7430d538f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6340019650>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "tabu1 = []\n",
    "tabu2 = []\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size_train = 512\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kN22sGD4O1Re"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f634ba0fe10>\n",
      "torch.Size([512, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.FashionMNIST('./data', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.FashionMNIST('./data', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "print(train_loader)\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z0Q1QfHSHGS6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.CIFAR100('./data', train=True, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.CIFAR100('./data', train=False, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# print(train_loader)\n",
    "# examples = enumerate(train_loader)\n",
    "# batch_idx, (example_data, example_targets) = next(examples)\n",
    "# print(example_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5591,
     "status": "ok",
     "timestamp": 1555138490633,
     "user": {
      "displayName": "Ittahaduzzaman Akash",
      "photoUrl": "https://lh3.googleusercontent.com/-YERuM7ONE4k/AAAAAAAAAAI/AAAAAAAAAM8/wlRqU6U6u18/s64/photo.jpg",
      "userId": "11972750857132575782"
     },
     "user_tz": -360
    },
    "id": "dm1RiT4cHGTK",
    "outputId": "43b41ab5-3dd9-43cf-e47f-5868110d3774"
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.CIFAR10('./data', train=True, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.CIFAR10('./data', train=False, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# print(train_loader)\n",
    "# examples = enumerate(train_loader)\n",
    "# batch_idx, (example_data, example_targets) = next(examples)\n",
    "# print(example_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PwEvmG00HGTY"
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.SVHN('./data', split='train', download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.SVHN('./data', split='test', download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# print(train_loader)\n",
    "# examples = enumerate(train_loader)\n",
    "# batch_idx, (example_data, example_targets) = next(examples)\n",
    "# print(example_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eV5zVHMxHGTj"
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.MNIST('./data', train=False, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# print(train_loader)\n",
    "# examples = enumerate(train_loader)\n",
    "# batch_idx, (example_data, example_targets) = next(examples)\n",
    "# print(example_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dl7nDBdAHGTw"
   },
   "outputs": [],
   "source": [
    "def initMatrix(size, append_with):\n",
    "    ary = []\n",
    "    for i in range(0, size):\n",
    "        ary.append(append_with)\n",
    "    return ary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LrjISTO9HGT6"
   },
   "outputs": [],
   "source": [
    "from torch.nn.modules import Module\n",
    "from torch.nn import functional as F\n",
    "from torch._jit_internal import weak_module, weak_script_method\n",
    "\n",
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5, inplace=False):\n",
    "        super(Dropout, self).__init__()\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"dropout probability has to be between 0 and 1, \"\n",
    "                             \"but got {}\".format(p))\n",
    "        self.p = p\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, input):\n",
    "        varTemp = F.dropout(input, self.p, self.training, self.inplace)\n",
    "        return varTemp\n",
    "\n",
    "    def __repr__(self):\n",
    "        inplace_str = ', inplace' if self.inplace else ''\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'p=' + str(self.p) \\\n",
    "            + inplace_str + ')'\n",
    "    \n",
    "class MyLinear(torch.nn.Linear):\n",
    "    def __init__(self, in_feats, out_feats, drop_p, t, bias=True):\n",
    "        super(MyLinear, self).__init__(in_feats, out_feats, bias=bias)\n",
    "        self.masker = Dropout(p=drop_p)\n",
    "        self.tabu = t\n",
    "        self.firstItr = True\n",
    "        self.network_dropout = drop_p\n",
    "        self.historial_tabu_count = initMatrix(1024, 0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print(self.tabu)\n",
    "        #print(input)\n",
    "        \n",
    "       \n",
    "        masked_weight = self.masker(self.weight)\n",
    "        #print(masked_weight)\n",
    "        output = F.linear(input, masked_weight, self.bias)\n",
    "        \n",
    "        # Here goes TABU\n",
    "        if self.firstItr == True:\n",
    "            \n",
    "            self.firstItr = False\n",
    "            for i in range(0, len(output[0])):\n",
    "                if output[0][i] < 0:\n",
    "                    # Neuron will be dropped\n",
    "                    self.tabu[i] = 0\n",
    "        else:\n",
    "            \n",
    "            temp_tabu = initMatrix(len(output[0]), 1)\n",
    "            \n",
    "            for i in range(0, len(output[0])):\n",
    "                if output[0][i] < 0:\n",
    "                    # Neuron will be dropped\n",
    "                    temp_tabu[i] = 0\n",
    "\n",
    "            for i in range(0, len(output[0])):\n",
    "                if (self.tabu[i] == 0 and temp_tabu[i] == 0):\n",
    "                    # Neuron will be dropped\n",
    "                    self.historial_tabu_count[i] = self.historial_tabu_count[i] + 1\n",
    "                    \n",
    "                    if self.historial_tabu_count[i] > 3:\n",
    "                      \n",
    "                      self.tabu[i] = 1\n",
    "                      output[0][i] = input[0][i]\n",
    "                      self.historial_tabu_count[i] = 0\n",
    "                else:\n",
    "                    self.tabu[i] = temp_tabu[i]\n",
    "                    \n",
    "            #print(temp_tabu)\n",
    "            \n",
    "        #print(self.historial_tabu_count)    \n",
    "     \n",
    "            \n",
    "        #print(output)\n",
    "        #print(self.tab5u)\n",
    "        \n",
    "        self.firstItr = False\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPm0tWSOHGUF"
   },
   "outputs": [],
   "source": [
    "tabu1 = initMatrix(1024, 1)\n",
    "tabu2 = initMatrix(1024, 1)\n",
    "\n",
    "layer1_dropout = 0.5\n",
    "layer2_dropout = 0.5\n",
    "\n",
    "#tabu3 = initMatrix(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ei-XUUvmHGUO"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "       \n",
    "        # define the layers and their sizes, turn off bias\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 1024)\n",
    "       \n",
    "        self.aD1 = MyLinear(1024, 1024, layer1_dropout, tabu1) #nn.Dropout(0.5) # # #      \n",
    "        \n",
    "        #self.d2 = nn.Dropout(0.5)\n",
    "        #self.aD2 = MyLinear(1024, 1024, layer2_dropout, tabu2) \n",
    "        #self.fc2 = nn.Linear(1024, 1024)\n",
    "        #self.d2 = Dropout(0.5) \n",
    "        #self.fc3 = nn.Linear(1024, 1024)\n",
    "        \n",
    "        #For CIFAR-10\n",
    "        #self.d3 = Dropout(0.3) \n",
    "        #self.fc3_1 = nn.Linear(512, 256)\n",
    "        \n",
    "        self.fc4 = nn.Linear(1024, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(\"First TABU Layer\")\n",
    "        x = F.relu(self.aD1(x))\n",
    "        #print(\"Second TABU Layer\")\n",
    "        #x = F.relu(self.dT(x))\n",
    "        #x = F.relu(self.aD2(x))\n",
    "        \n",
    "        #x = F.relu(self.d2(x))\n",
    "        #x = F.relu(self.fc3(x))\n",
    "        \n",
    "        # for CIFAR-10\n",
    "        #x = F.relu(self.d3(x))\n",
    "        #x = F.relu(self.fc3_1(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9jp8qjX7HGUa"
   },
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HgE1I92yHGVA"
   },
   "outputs": [],
   "source": [
    "avg_train_loss = []\n",
    "avg_train_counter = []\n",
    "avg_test_loss = []\n",
    "avg_test_counter = []\n",
    "test_accuracy_list = []\n",
    "\n",
    "test_losses = []\n",
    "test_counter = []\n",
    "\n",
    "epoch_number = 0\n",
    "train_avg_loss = 0\n",
    "test_avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aGOcLvQ2HGVG"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  \n",
    "    train_losses = []\n",
    "    train_counter = []\n",
    "    \n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            torch.save(network.state_dict(), './results/fasionmnist_with_avgtabu_model.pth')\n",
    "            torch.save(optimizer.state_dict(), './results/fasionmnist_with_avgtabu_optimizer.pth')\n",
    "            #test(epoch, False)\n",
    "            \n",
    "    sum = 0.0\n",
    "    for loss in train_losses:\n",
    "      sum = sum + loss\n",
    "    \n",
    "    avg_train_loss.append(sum / len(train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwqm3AuCHGVM"
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "        #iterator = iter(test_loader)\n",
    "        #data, target= iterator.next() \n",
    "        \n",
    "            examples = enumerate(test_loader)\n",
    "            batch_idx, (example_data, example_targets) = next(examples)  \n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    avg_test_loss.append(test_loss)\n",
    "    test_counter.append((batch_idx*64) + ((epoch-1)*len(test_loader.dataset)))\n",
    "    test_accuracy_list.append(100. * correct / len(test_loader.dataset))\n",
    "    \n",
    "    print('Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2536
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1093203,
     "status": "ok",
     "timestamp": 1555083207914,
     "user": {
      "displayName": "Ittahaduzzaman Akash",
      "photoUrl": "https://lh3.googleusercontent.com/-YERuM7ONE4k/AAAAAAAAAAI/AAAAAAAAAM8/wlRqU6U6u18/s64/photo.jpg",
      "userId": "11972750857132575782"
     },
     "user_tz": -360
    },
    "id": "77ZTDVwnHGVS",
    "outputId": "0ceebe6d-b3a4-46a0-8c2b-7f4303483729",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.327716\n",
      "Train Epoch: 1 [5120/60000 (8%)]\tLoss: 2.203989\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.080095\n",
      "Train Epoch: 1 [15360/60000 (25%)]\tLoss: 1.960088\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.841771\n",
      "Train Epoch: 1 [25600/60000 (42%)]\tLoss: 1.706372\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.561395\n",
      "Train Epoch: 1 [35840/60000 (59%)]\tLoss: 1.497776\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.382727\n",
      "Train Epoch: 1 [46080/60000 (76%)]\tLoss: 1.328371\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.230072\n",
      "Train Epoch: 1 [56320/60000 (93%)]\tLoss: 1.211658\n",
      "Test set: Avg. loss: 1.0698, Accuracy: 6498/10000 (64%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.111567\n",
      "Train Epoch: 2 [5120/60000 (8%)]\tLoss: 1.116737\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 1.015146\n",
      "Train Epoch: 2 [15360/60000 (25%)]\tLoss: 0.990579\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.979843\n",
      "Train Epoch: 2 [25600/60000 (42%)]\tLoss: 0.949116\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.938405\n",
      "Train Epoch: 2 [35840/60000 (59%)]\tLoss: 0.864961\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.854351\n",
      "Train Epoch: 2 [46080/60000 (76%)]\tLoss: 0.956674\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.823752\n",
      "Train Epoch: 2 [56320/60000 (93%)]\tLoss: 0.823780\n",
      "Test set: Avg. loss: 0.8040, Accuracy: 7157/10000 (71%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.856456\n",
      "Train Epoch: 3 [5120/60000 (8%)]\tLoss: 0.815087\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.850987\n",
      "Train Epoch: 3 [15360/60000 (25%)]\tLoss: 0.727796\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.793912\n",
      "Train Epoch: 3 [25600/60000 (42%)]\tLoss: 0.694511\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.723980\n",
      "Train Epoch: 3 [35840/60000 (59%)]\tLoss: 0.701487\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.692006\n",
      "Train Epoch: 3 [46080/60000 (76%)]\tLoss: 0.812854\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.704128\n",
      "Train Epoch: 3 [56320/60000 (93%)]\tLoss: 0.748051\n",
      "Test set: Avg. loss: 0.7059, Accuracy: 7459/10000 (74%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.667661\n",
      "Train Epoch: 4 [5120/60000 (8%)]\tLoss: 0.671463\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.650760\n",
      "Train Epoch: 4 [15360/60000 (25%)]\tLoss: 0.656640\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.731999\n",
      "Train Epoch: 4 [25600/60000 (42%)]\tLoss: 0.712745\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.663042\n",
      "Train Epoch: 4 [35840/60000 (59%)]\tLoss: 0.662337\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.691625\n",
      "Train Epoch: 4 [46080/60000 (76%)]\tLoss: 0.641648\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.692552\n",
      "Train Epoch: 4 [56320/60000 (93%)]\tLoss: 0.642685\n",
      "Test set: Avg. loss: 0.6570, Accuracy: 7608/10000 (76%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.702766\n",
      "Train Epoch: 5 [5120/60000 (8%)]\tLoss: 0.663902\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.706691\n",
      "Train Epoch: 5 [15360/60000 (25%)]\tLoss: 0.633805\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.611363\n",
      "Train Epoch: 5 [25600/60000 (42%)]\tLoss: 0.624644\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.606250\n",
      "Train Epoch: 5 [35840/60000 (59%)]\tLoss: 0.577707\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.610710\n",
      "Train Epoch: 5 [46080/60000 (76%)]\tLoss: 0.546229\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.553391\n",
      "Train Epoch: 5 [56320/60000 (93%)]\tLoss: 0.558402\n",
      "Test set: Avg. loss: 0.6106, Accuracy: 7824/10000 (78%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.609855\n",
      "Train Epoch: 6 [5120/60000 (8%)]\tLoss: 0.633153\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.576485\n",
      "Train Epoch: 6 [15360/60000 (25%)]\tLoss: 0.619754\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.581178\n",
      "Train Epoch: 6 [25600/60000 (42%)]\tLoss: 0.594954\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.616737\n",
      "Train Epoch: 6 [35840/60000 (59%)]\tLoss: 0.536004\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.538687\n",
      "Train Epoch: 6 [46080/60000 (76%)]\tLoss: 0.523631\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.595714\n",
      "Train Epoch: 6 [56320/60000 (93%)]\tLoss: 0.575755\n",
      "Test set: Avg. loss: 0.5774, Accuracy: 7933/10000 (79%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.526136\n",
      "Train Epoch: 7 [5120/60000 (8%)]\tLoss: 0.554790\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.611631\n",
      "Train Epoch: 7 [15360/60000 (25%)]\tLoss: 0.544814\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.591697\n",
      "Train Epoch: 7 [25600/60000 (42%)]\tLoss: 0.540549\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.668047\n",
      "Train Epoch: 7 [35840/60000 (59%)]\tLoss: 0.508783\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.568082\n",
      "Train Epoch: 7 [46080/60000 (76%)]\tLoss: 0.531911\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.503996\n",
      "Train Epoch: 7 [56320/60000 (93%)]\tLoss: 0.564444\n",
      "Test set: Avg. loss: 0.5571, Accuracy: 7993/10000 (79%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.572914\n",
      "Train Epoch: 8 [5120/60000 (8%)]\tLoss: 0.498322\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.567255\n",
      "Train Epoch: 8 [15360/60000 (25%)]\tLoss: 0.548016\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.526953\n",
      "Train Epoch: 8 [25600/60000 (42%)]\tLoss: 0.507084\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.618149\n",
      "Train Epoch: 8 [35840/60000 (59%)]\tLoss: 0.484430\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.568418\n",
      "Train Epoch: 8 [46080/60000 (76%)]\tLoss: 0.598947\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.528858\n",
      "Train Epoch: 8 [56320/60000 (93%)]\tLoss: 0.500712\n",
      "Test set: Avg. loss: 0.5345, Accuracy: 8098/10000 (80%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.500608\n",
      "Train Epoch: 9 [5120/60000 (8%)]\tLoss: 0.553490\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.529935\n",
      "Train Epoch: 9 [15360/60000 (25%)]\tLoss: 0.502016\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.493459\n",
      "Train Epoch: 9 [25600/60000 (42%)]\tLoss: 0.584282\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.558255\n",
      "Train Epoch: 9 [35840/60000 (59%)]\tLoss: 0.533286\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.488083\n",
      "Train Epoch: 9 [46080/60000 (76%)]\tLoss: 0.507409\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.554692\n",
      "Train Epoch: 9 [56320/60000 (93%)]\tLoss: 0.567540\n",
      "Test set: Avg. loss: 0.5269, Accuracy: 8132/10000 (81%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.545595\n",
      "Train Epoch: 10 [5120/60000 (8%)]\tLoss: 0.529842\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.475178\n",
      "Train Epoch: 10 [15360/60000 (25%)]\tLoss: 0.523851\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.552102\n",
      "Train Epoch: 10 [25600/60000 (42%)]\tLoss: 0.501976\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.532945\n",
      "Train Epoch: 10 [35840/60000 (59%)]\tLoss: 0.466029\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.415707\n",
      "Train Epoch: 10 [46080/60000 (76%)]\tLoss: 0.439965\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.571473\n",
      "Train Epoch: 10 [56320/60000 (93%)]\tLoss: 0.511582\n",
      "Test set: Avg. loss: 0.5220, Accuracy: 8134/10000 (81%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.455901\n",
      "Train Epoch: 11 [5120/60000 (8%)]\tLoss: 0.520375\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.469486\n",
      "Train Epoch: 11 [15360/60000 (25%)]\tLoss: 0.508511\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.555775\n",
      "Train Epoch: 11 [25600/60000 (42%)]\tLoss: 0.490486\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.559154\n",
      "Train Epoch: 11 [35840/60000 (59%)]\tLoss: 0.425836\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.487074\n",
      "Train Epoch: 11 [46080/60000 (76%)]\tLoss: 0.462341\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.462885\n",
      "Train Epoch: 11 [56320/60000 (93%)]\tLoss: 0.518716\n",
      "Test set: Avg. loss: 0.5070, Accuracy: 8192/10000 (81%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.488701\n",
      "Train Epoch: 12 [5120/60000 (8%)]\tLoss: 0.490859\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.464789\n",
      "Train Epoch: 12 [15360/60000 (25%)]\tLoss: 0.466168\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.557947\n",
      "Train Epoch: 12 [25600/60000 (42%)]\tLoss: 0.529154\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.439851\n",
      "Train Epoch: 12 [35840/60000 (59%)]\tLoss: 0.490879\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.409703\n",
      "Train Epoch: 12 [46080/60000 (76%)]\tLoss: 0.475039\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.442797\n",
      "Train Epoch: 12 [56320/60000 (93%)]\tLoss: 0.479456\n",
      "Test set: Avg. loss: 0.5016, Accuracy: 8205/10000 (82%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.452220\n",
      "Train Epoch: 13 [5120/60000 (8%)]\tLoss: 0.546733\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.533154\n",
      "Train Epoch: 13 [15360/60000 (25%)]\tLoss: 0.413130\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.512056\n",
      "Train Epoch: 13 [25600/60000 (42%)]\tLoss: 0.451073\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.438196\n",
      "Train Epoch: 13 [35840/60000 (59%)]\tLoss: 0.431359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.470997\n",
      "Train Epoch: 13 [46080/60000 (76%)]\tLoss: 0.462300\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.474416\n",
      "Train Epoch: 13 [56320/60000 (93%)]\tLoss: 0.500268\n",
      "Test set: Avg. loss: 0.5044, Accuracy: 8209/10000 (82%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.493157\n",
      "Train Epoch: 14 [5120/60000 (8%)]\tLoss: 0.415811\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.431457\n",
      "Train Epoch: 14 [15360/60000 (25%)]\tLoss: 0.492604\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.528611\n",
      "Train Epoch: 14 [25600/60000 (42%)]\tLoss: 0.481787\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.526731\n",
      "Train Epoch: 14 [35840/60000 (59%)]\tLoss: 0.467450\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.394394\n",
      "Train Epoch: 14 [46080/60000 (76%)]\tLoss: 0.455277\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.452071\n",
      "Train Epoch: 14 [56320/60000 (93%)]\tLoss: 0.558289\n",
      "Test set: Avg. loss: 0.4856, Accuracy: 8280/10000 (82%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.430698\n",
      "Train Epoch: 15 [5120/60000 (8%)]\tLoss: 0.446263\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.443384\n",
      "Train Epoch: 15 [15360/60000 (25%)]\tLoss: 0.392290\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.479870\n",
      "Train Epoch: 15 [25600/60000 (42%)]\tLoss: 0.509754\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.406919\n",
      "Train Epoch: 15 [35840/60000 (59%)]\tLoss: 0.449257\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.432158\n",
      "Train Epoch: 15 [46080/60000 (76%)]\tLoss: 0.438463\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.434028\n",
      "Train Epoch: 15 [56320/60000 (93%)]\tLoss: 0.506186\n",
      "Test set: Avg. loss: 0.5342, Accuracy: 8093/10000 (80%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.506684\n",
      "Train Epoch: 16 [5120/60000 (8%)]\tLoss: 0.526431\n",
      "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.478409\n",
      "Train Epoch: 16 [15360/60000 (25%)]\tLoss: 0.447927\n",
      "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.467969\n",
      "Train Epoch: 16 [25600/60000 (42%)]\tLoss: 0.444287\n",
      "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.451679\n",
      "Train Epoch: 16 [35840/60000 (59%)]\tLoss: 0.416956\n",
      "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.465509\n",
      "Train Epoch: 16 [46080/60000 (76%)]\tLoss: 0.380861\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.512744\n",
      "Train Epoch: 16 [56320/60000 (93%)]\tLoss: 0.442092\n",
      "Test set: Avg. loss: 0.4749, Accuracy: 8317/10000 (83%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.405049\n",
      "Train Epoch: 17 [5120/60000 (8%)]\tLoss: 0.443231\n",
      "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.447937\n",
      "Train Epoch: 17 [15360/60000 (25%)]\tLoss: 0.466930\n",
      "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.514935\n",
      "Train Epoch: 17 [25600/60000 (42%)]\tLoss: 0.463189\n",
      "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.502151\n",
      "Train Epoch: 17 [35840/60000 (59%)]\tLoss: 0.396007\n",
      "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.450721\n",
      "Train Epoch: 17 [46080/60000 (76%)]\tLoss: 0.479932\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.495117\n",
      "Train Epoch: 17 [56320/60000 (93%)]\tLoss: 0.509183\n",
      "Test set: Avg. loss: 0.4702, Accuracy: 8331/10000 (83%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.379164\n",
      "Train Epoch: 18 [5120/60000 (8%)]\tLoss: 0.543079\n",
      "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 0.451723\n",
      "Train Epoch: 18 [15360/60000 (25%)]\tLoss: 0.409302\n",
      "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 0.492694\n",
      "Train Epoch: 18 [25600/60000 (42%)]\tLoss: 0.423743\n",
      "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 0.460848\n",
      "Train Epoch: 18 [35840/60000 (59%)]\tLoss: 0.386956\n",
      "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 0.399103\n",
      "Train Epoch: 18 [46080/60000 (76%)]\tLoss: 0.451015\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.390648\n",
      "Train Epoch: 18 [56320/60000 (93%)]\tLoss: 0.429291\n",
      "Test set: Avg. loss: 0.4601, Accuracy: 8353/10000 (83%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.457209\n",
      "Train Epoch: 19 [5120/60000 (8%)]\tLoss: 0.438856\n",
      "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 0.446311\n",
      "Train Epoch: 19 [15360/60000 (25%)]\tLoss: 0.434801\n",
      "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 0.365434\n",
      "Train Epoch: 19 [25600/60000 (42%)]\tLoss: 0.416065\n",
      "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 0.463567\n",
      "Train Epoch: 19 [35840/60000 (59%)]\tLoss: 0.433705\n",
      "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 0.453318\n",
      "Train Epoch: 19 [46080/60000 (76%)]\tLoss: 0.469923\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.490072\n",
      "Train Epoch: 19 [56320/60000 (93%)]\tLoss: 0.460842\n",
      "Test set: Avg. loss: 0.4659, Accuracy: 8335/10000 (83%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.432017\n",
      "Train Epoch: 20 [5120/60000 (8%)]\tLoss: 0.433390\n",
      "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 0.397112\n",
      "Train Epoch: 20 [15360/60000 (25%)]\tLoss: 0.458312\n",
      "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 0.376745\n",
      "Train Epoch: 20 [25600/60000 (42%)]\tLoss: 0.431666\n",
      "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 0.410557\n",
      "Train Epoch: 20 [35840/60000 (59%)]\tLoss: 0.417537\n",
      "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 0.501810\n",
      "Train Epoch: 20 [46080/60000 (76%)]\tLoss: 0.430946\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.440944\n",
      "Train Epoch: 20 [56320/60000 (93%)]\tLoss: 0.370884\n",
      "Test set: Avg. loss: 0.4524, Accuracy: 8380/10000 (83%)\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.472486\n",
      "Train Epoch: 21 [5120/60000 (8%)]\tLoss: 0.422512\n",
      "Train Epoch: 21 [10240/60000 (17%)]\tLoss: 0.407327\n",
      "Train Epoch: 21 [15360/60000 (25%)]\tLoss: 0.427111\n",
      "Train Epoch: 21 [20480/60000 (34%)]\tLoss: 0.369083\n",
      "Train Epoch: 21 [25600/60000 (42%)]\tLoss: 0.453980\n",
      "Train Epoch: 21 [30720/60000 (51%)]\tLoss: 0.411504\n",
      "Train Epoch: 21 [35840/60000 (59%)]\tLoss: 0.416579\n",
      "Train Epoch: 21 [40960/60000 (68%)]\tLoss: 0.450165\n",
      "Train Epoch: 21 [46080/60000 (76%)]\tLoss: 0.409101\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.387136\n",
      "Train Epoch: 21 [56320/60000 (93%)]\tLoss: 0.376440\n",
      "Test set: Avg. loss: 0.4458, Accuracy: 8428/10000 (84%)\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.412624\n",
      "Train Epoch: 22 [5120/60000 (8%)]\tLoss: 0.390904\n",
      "Train Epoch: 22 [10240/60000 (17%)]\tLoss: 0.496126\n",
      "Train Epoch: 22 [15360/60000 (25%)]\tLoss: 0.436865\n",
      "Train Epoch: 22 [20480/60000 (34%)]\tLoss: 0.449987\n",
      "Train Epoch: 22 [25600/60000 (42%)]\tLoss: 0.433040\n",
      "Train Epoch: 22 [30720/60000 (51%)]\tLoss: 0.408757\n",
      "Train Epoch: 22 [35840/60000 (59%)]\tLoss: 0.477776\n",
      "Train Epoch: 22 [40960/60000 (68%)]\tLoss: 0.505939\n",
      "Train Epoch: 22 [46080/60000 (76%)]\tLoss: 0.443819\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.427591\n",
      "Train Epoch: 22 [56320/60000 (93%)]\tLoss: 0.423811\n",
      "Test set: Avg. loss: 0.4419, Accuracy: 8439/10000 (84%)\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.436182\n",
      "Train Epoch: 23 [5120/60000 (8%)]\tLoss: 0.428400\n",
      "Train Epoch: 23 [10240/60000 (17%)]\tLoss: 0.447705\n",
      "Train Epoch: 23 [15360/60000 (25%)]\tLoss: 0.457664\n",
      "Train Epoch: 23 [20480/60000 (34%)]\tLoss: 0.373331\n",
      "Train Epoch: 23 [25600/60000 (42%)]\tLoss: 0.343249\n",
      "Train Epoch: 23 [30720/60000 (51%)]\tLoss: 0.377669\n",
      "Train Epoch: 23 [35840/60000 (59%)]\tLoss: 0.422076\n",
      "Train Epoch: 23 [40960/60000 (68%)]\tLoss: 0.462034\n",
      "Train Epoch: 23 [46080/60000 (76%)]\tLoss: 0.431673\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.435236\n",
      "Train Epoch: 23 [56320/60000 (93%)]\tLoss: 0.503211\n",
      "Test set: Avg. loss: 0.4408, Accuracy: 8424/10000 (84%)\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.468395\n",
      "Train Epoch: 24 [5120/60000 (8%)]\tLoss: 0.420537\n",
      "Train Epoch: 24 [10240/60000 (17%)]\tLoss: 0.401716\n",
      "Train Epoch: 24 [15360/60000 (25%)]\tLoss: 0.396441\n",
      "Train Epoch: 24 [20480/60000 (34%)]\tLoss: 0.415033\n",
      "Train Epoch: 24 [25600/60000 (42%)]\tLoss: 0.468209\n",
      "Train Epoch: 24 [30720/60000 (51%)]\tLoss: 0.379132\n",
      "Train Epoch: 24 [35840/60000 (59%)]\tLoss: 0.344133\n",
      "Train Epoch: 24 [40960/60000 (68%)]\tLoss: 0.419380\n",
      "Train Epoch: 24 [46080/60000 (76%)]\tLoss: 0.423550\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.460974\n",
      "Train Epoch: 24 [56320/60000 (93%)]\tLoss: 0.470803\n",
      "Test set: Avg. loss: 0.4413, Accuracy: 8416/10000 (84%)\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.465423\n",
      "Train Epoch: 25 [5120/60000 (8%)]\tLoss: 0.385990\n",
      "Train Epoch: 25 [10240/60000 (17%)]\tLoss: 0.379559\n",
      "Train Epoch: 25 [15360/60000 (25%)]\tLoss: 0.392952\n",
      "Train Epoch: 25 [20480/60000 (34%)]\tLoss: 0.420188\n",
      "Train Epoch: 25 [25600/60000 (42%)]\tLoss: 0.412135\n",
      "Train Epoch: 25 [30720/60000 (51%)]\tLoss: 0.450398\n",
      "Train Epoch: 25 [35840/60000 (59%)]\tLoss: 0.394498\n",
      "Train Epoch: 25 [40960/60000 (68%)]\tLoss: 0.399714\n",
      "Train Epoch: 25 [46080/60000 (76%)]\tLoss: 0.394797\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.375575\n",
      "Train Epoch: 25 [56320/60000 (93%)]\tLoss: 0.432071\n",
      "Test set: Avg. loss: 0.4447, Accuracy: 8388/10000 (83%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.433020\n",
      "Train Epoch: 26 [5120/60000 (8%)]\tLoss: 0.394932\n",
      "Train Epoch: 26 [10240/60000 (17%)]\tLoss: 0.516940\n",
      "Train Epoch: 26 [15360/60000 (25%)]\tLoss: 0.393176\n",
      "Train Epoch: 26 [20480/60000 (34%)]\tLoss: 0.422760\n",
      "Train Epoch: 26 [25600/60000 (42%)]\tLoss: 0.413608\n",
      "Train Epoch: 26 [30720/60000 (51%)]\tLoss: 0.383192\n",
      "Train Epoch: 26 [35840/60000 (59%)]\tLoss: 0.416155\n",
      "Train Epoch: 26 [40960/60000 (68%)]\tLoss: 0.416521\n",
      "Train Epoch: 26 [46080/60000 (76%)]\tLoss: 0.358036\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.349583\n",
      "Train Epoch: 26 [56320/60000 (93%)]\tLoss: 0.394873\n",
      "Test set: Avg. loss: 0.4415, Accuracy: 8423/10000 (84%)\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.383282\n",
      "Train Epoch: 27 [5120/60000 (8%)]\tLoss: 0.358825\n",
      "Train Epoch: 27 [10240/60000 (17%)]\tLoss: 0.376326\n",
      "Train Epoch: 27 [15360/60000 (25%)]\tLoss: 0.375665\n",
      "Train Epoch: 27 [20480/60000 (34%)]\tLoss: 0.373357\n",
      "Train Epoch: 27 [25600/60000 (42%)]\tLoss: 0.360027\n",
      "Train Epoch: 27 [30720/60000 (51%)]\tLoss: 0.417197\n",
      "Train Epoch: 27 [35840/60000 (59%)]\tLoss: 0.400476\n",
      "Train Epoch: 27 [40960/60000 (68%)]\tLoss: 0.412028\n",
      "Train Epoch: 27 [46080/60000 (76%)]\tLoss: 0.394222\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.481713\n",
      "Train Epoch: 27 [56320/60000 (93%)]\tLoss: 0.353747\n",
      "Test set: Avg. loss: 0.4279, Accuracy: 8475/10000 (84%)\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.371735\n",
      "Train Epoch: 28 [5120/60000 (8%)]\tLoss: 0.494020\n",
      "Train Epoch: 28 [10240/60000 (17%)]\tLoss: 0.457209\n",
      "Train Epoch: 28 [15360/60000 (25%)]\tLoss: 0.396996\n",
      "Train Epoch: 28 [20480/60000 (34%)]\tLoss: 0.371375\n",
      "Train Epoch: 28 [25600/60000 (42%)]\tLoss: 0.463760\n",
      "Train Epoch: 28 [30720/60000 (51%)]\tLoss: 0.394605\n",
      "Train Epoch: 28 [35840/60000 (59%)]\tLoss: 0.423230\n",
      "Train Epoch: 28 [40960/60000 (68%)]\tLoss: 0.420800\n",
      "Train Epoch: 28 [46080/60000 (76%)]\tLoss: 0.375716\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.359661\n",
      "Train Epoch: 28 [56320/60000 (93%)]\tLoss: 0.383698\n",
      "Test set: Avg. loss: 0.4352, Accuracy: 8442/10000 (84%)\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.512092\n",
      "Train Epoch: 29 [5120/60000 (8%)]\tLoss: 0.376322\n",
      "Train Epoch: 29 [10240/60000 (17%)]\tLoss: 0.380819\n",
      "Train Epoch: 29 [15360/60000 (25%)]\tLoss: 0.338364\n",
      "Train Epoch: 29 [20480/60000 (34%)]\tLoss: 0.379126\n",
      "Train Epoch: 29 [25600/60000 (42%)]\tLoss: 0.406027\n",
      "Train Epoch: 29 [30720/60000 (51%)]\tLoss: 0.398948\n",
      "Train Epoch: 29 [35840/60000 (59%)]\tLoss: 0.387890\n",
      "Train Epoch: 29 [40960/60000 (68%)]\tLoss: 0.386064\n",
      "Train Epoch: 29 [46080/60000 (76%)]\tLoss: 0.388564\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.392935\n",
      "Train Epoch: 29 [56320/60000 (93%)]\tLoss: 0.402139\n",
      "Test set: Avg. loss: 0.4269, Accuracy: 8480/10000 (84%)\n",
      "\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.436465\n",
      "Train Epoch: 30 [5120/60000 (8%)]\tLoss: 0.353298\n",
      "Train Epoch: 30 [10240/60000 (17%)]\tLoss: 0.410174\n",
      "Train Epoch: 30 [15360/60000 (25%)]\tLoss: 0.360370\n",
      "Train Epoch: 30 [20480/60000 (34%)]\tLoss: 0.400235\n",
      "Train Epoch: 30 [25600/60000 (42%)]\tLoss: 0.386320\n",
      "Train Epoch: 30 [30720/60000 (51%)]\tLoss: 0.350107\n",
      "Train Epoch: 30 [35840/60000 (59%)]\tLoss: 0.436879\n",
      "Train Epoch: 30 [40960/60000 (68%)]\tLoss: 0.402606\n",
      "Train Epoch: 30 [46080/60000 (76%)]\tLoss: 0.375862\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.403287\n",
      "Train Epoch: 30 [56320/60000 (93%)]\tLoss: 0.403312\n",
      "Test set: Avg. loss: 0.4349, Accuracy: 8444/10000 (84%)\n",
      "\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.406272\n",
      "Train Epoch: 31 [5120/60000 (8%)]\tLoss: 0.361107\n",
      "Train Epoch: 31 [10240/60000 (17%)]\tLoss: 0.399492\n",
      "Train Epoch: 31 [15360/60000 (25%)]\tLoss: 0.387935\n",
      "Train Epoch: 31 [20480/60000 (34%)]\tLoss: 0.401783\n",
      "Train Epoch: 31 [25600/60000 (42%)]\tLoss: 0.404743\n",
      "Train Epoch: 31 [30720/60000 (51%)]\tLoss: 0.329410\n",
      "Train Epoch: 31 [35840/60000 (59%)]\tLoss: 0.317671\n",
      "Train Epoch: 31 [40960/60000 (68%)]\tLoss: 0.417482\n",
      "Train Epoch: 31 [46080/60000 (76%)]\tLoss: 0.392013\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.411346\n",
      "Train Epoch: 31 [56320/60000 (93%)]\tLoss: 0.423957\n",
      "Test set: Avg. loss: 0.4236, Accuracy: 8493/10000 (84%)\n",
      "\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.383431\n",
      "Train Epoch: 32 [5120/60000 (8%)]\tLoss: 0.374632\n",
      "Train Epoch: 32 [10240/60000 (17%)]\tLoss: 0.364242\n",
      "Train Epoch: 32 [15360/60000 (25%)]\tLoss: 0.417591\n",
      "Train Epoch: 32 [20480/60000 (34%)]\tLoss: 0.341955\n",
      "Train Epoch: 32 [25600/60000 (42%)]\tLoss: 0.346473\n",
      "Train Epoch: 32 [30720/60000 (51%)]\tLoss: 0.394586\n",
      "Train Epoch: 32 [35840/60000 (59%)]\tLoss: 0.343313\n",
      "Train Epoch: 32 [40960/60000 (68%)]\tLoss: 0.459336\n",
      "Train Epoch: 32 [46080/60000 (76%)]\tLoss: 0.388461\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.376827\n",
      "Train Epoch: 32 [56320/60000 (93%)]\tLoss: 0.362305\n",
      "Test set: Avg. loss: 0.4137, Accuracy: 8512/10000 (85%)\n",
      "\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.360489\n",
      "Train Epoch: 33 [5120/60000 (8%)]\tLoss: 0.356638\n",
      "Train Epoch: 33 [10240/60000 (17%)]\tLoss: 0.419724\n",
      "Train Epoch: 33 [15360/60000 (25%)]\tLoss: 0.387999\n",
      "Train Epoch: 33 [20480/60000 (34%)]\tLoss: 0.370086\n",
      "Train Epoch: 33 [25600/60000 (42%)]\tLoss: 0.434976\n",
      "Train Epoch: 33 [30720/60000 (51%)]\tLoss: 0.342131\n",
      "Train Epoch: 33 [35840/60000 (59%)]\tLoss: 0.354941\n",
      "Train Epoch: 33 [40960/60000 (68%)]\tLoss: 0.429771\n",
      "Train Epoch: 33 [46080/60000 (76%)]\tLoss: 0.426524\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.364563\n",
      "Train Epoch: 33 [56320/60000 (93%)]\tLoss: 0.379693\n",
      "Test set: Avg. loss: 0.4132, Accuracy: 8513/10000 (85%)\n",
      "\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.390977\n",
      "Train Epoch: 34 [5120/60000 (8%)]\tLoss: 0.385544\n",
      "Train Epoch: 34 [10240/60000 (17%)]\tLoss: 0.360790\n",
      "Train Epoch: 34 [15360/60000 (25%)]\tLoss: 0.379606\n",
      "Train Epoch: 34 [20480/60000 (34%)]\tLoss: 0.391708\n",
      "Train Epoch: 34 [25600/60000 (42%)]\tLoss: 0.354239\n",
      "Train Epoch: 34 [30720/60000 (51%)]\tLoss: 0.401153\n",
      "Train Epoch: 34 [35840/60000 (59%)]\tLoss: 0.426990\n",
      "Train Epoch: 34 [40960/60000 (68%)]\tLoss: 0.415095\n",
      "Train Epoch: 34 [46080/60000 (76%)]\tLoss: 0.438917\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.417192\n",
      "Train Epoch: 34 [56320/60000 (93%)]\tLoss: 0.337716\n",
      "Test set: Avg. loss: 0.4089, Accuracy: 8533/10000 (85%)\n",
      "\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.388886\n",
      "Train Epoch: 35 [5120/60000 (8%)]\tLoss: 0.446716\n",
      "Train Epoch: 35 [10240/60000 (17%)]\tLoss: 0.392396\n",
      "Train Epoch: 35 [15360/60000 (25%)]\tLoss: 0.363583\n",
      "Train Epoch: 35 [20480/60000 (34%)]\tLoss: 0.366035\n",
      "Train Epoch: 35 [25600/60000 (42%)]\tLoss: 0.361630\n",
      "Train Epoch: 35 [30720/60000 (51%)]\tLoss: 0.351348\n",
      "Train Epoch: 35 [35840/60000 (59%)]\tLoss: 0.333441\n",
      "Train Epoch: 35 [40960/60000 (68%)]\tLoss: 0.345114\n",
      "Train Epoch: 35 [46080/60000 (76%)]\tLoss: 0.400094\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.359701\n",
      "Train Epoch: 35 [56320/60000 (93%)]\tLoss: 0.418951\n",
      "Test set: Avg. loss: 0.4083, Accuracy: 8518/10000 (85%)\n",
      "\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.371266\n",
      "Train Epoch: 36 [5120/60000 (8%)]\tLoss: 0.396658\n",
      "Train Epoch: 36 [10240/60000 (17%)]\tLoss: 0.415390\n",
      "Train Epoch: 36 [15360/60000 (25%)]\tLoss: 0.400605\n",
      "Train Epoch: 36 [20480/60000 (34%)]\tLoss: 0.377396\n",
      "Train Epoch: 36 [25600/60000 (42%)]\tLoss: 0.367910\n",
      "Train Epoch: 36 [30720/60000 (51%)]\tLoss: 0.453185\n",
      "Train Epoch: 36 [35840/60000 (59%)]\tLoss: 0.386369\n",
      "Train Epoch: 36 [40960/60000 (68%)]\tLoss: 0.447032\n",
      "Train Epoch: 36 [46080/60000 (76%)]\tLoss: 0.308101\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.355148\n",
      "Train Epoch: 36 [56320/60000 (93%)]\tLoss: 0.364932\n",
      "Test set: Avg. loss: 0.4061, Accuracy: 8546/10000 (85%)\n",
      "\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.331896\n",
      "Train Epoch: 37 [5120/60000 (8%)]\tLoss: 0.421504\n",
      "Train Epoch: 37 [10240/60000 (17%)]\tLoss: 0.287972\n",
      "Train Epoch: 37 [15360/60000 (25%)]\tLoss: 0.359483\n",
      "Train Epoch: 37 [20480/60000 (34%)]\tLoss: 0.379364\n",
      "Train Epoch: 37 [25600/60000 (42%)]\tLoss: 0.395482\n",
      "Train Epoch: 37 [30720/60000 (51%)]\tLoss: 0.364293\n",
      "Train Epoch: 37 [35840/60000 (59%)]\tLoss: 0.400835\n",
      "Train Epoch: 37 [40960/60000 (68%)]\tLoss: 0.409811\n",
      "Train Epoch: 37 [46080/60000 (76%)]\tLoss: 0.397408\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.353965\n",
      "Train Epoch: 37 [56320/60000 (93%)]\tLoss: 0.325018\n",
      "Test set: Avg. loss: 0.4193, Accuracy: 8509/10000 (85%)\n",
      "\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.357095\n",
      "Train Epoch: 38 [5120/60000 (8%)]\tLoss: 0.341490\n",
      "Train Epoch: 38 [10240/60000 (17%)]\tLoss: 0.366815\n",
      "Train Epoch: 38 [15360/60000 (25%)]\tLoss: 0.423688\n",
      "Train Epoch: 38 [20480/60000 (34%)]\tLoss: 0.345710\n",
      "Train Epoch: 38 [25600/60000 (42%)]\tLoss: 0.376140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 38 [30720/60000 (51%)]\tLoss: 0.342234\n",
      "Train Epoch: 38 [35840/60000 (59%)]\tLoss: 0.336083\n",
      "Train Epoch: 38 [40960/60000 (68%)]\tLoss: 0.442028\n",
      "Train Epoch: 38 [46080/60000 (76%)]\tLoss: 0.363686\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.349991\n",
      "Train Epoch: 38 [56320/60000 (93%)]\tLoss: 0.365001\n",
      "Test set: Avg. loss: 0.4019, Accuracy: 8547/10000 (85%)\n",
      "\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.357758\n",
      "Train Epoch: 39 [5120/60000 (8%)]\tLoss: 0.405228\n",
      "Train Epoch: 39 [10240/60000 (17%)]\tLoss: 0.378381\n",
      "Train Epoch: 39 [15360/60000 (25%)]\tLoss: 0.373981\n",
      "Train Epoch: 39 [20480/60000 (34%)]\tLoss: 0.348313\n",
      "Train Epoch: 39 [25600/60000 (42%)]\tLoss: 0.350801\n",
      "Train Epoch: 39 [30720/60000 (51%)]\tLoss: 0.432669\n",
      "Train Epoch: 39 [35840/60000 (59%)]\tLoss: 0.382675\n",
      "Train Epoch: 39 [40960/60000 (68%)]\tLoss: 0.352473\n",
      "Train Epoch: 39 [46080/60000 (76%)]\tLoss: 0.362398\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.348603\n",
      "Train Epoch: 39 [56320/60000 (93%)]\tLoss: 0.375049\n",
      "Test set: Avg. loss: 0.4024, Accuracy: 8570/10000 (85%)\n",
      "\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.367022\n",
      "Train Epoch: 40 [5120/60000 (8%)]\tLoss: 0.371018\n",
      "Train Epoch: 40 [10240/60000 (17%)]\tLoss: 0.328090\n",
      "Train Epoch: 40 [15360/60000 (25%)]\tLoss: 0.364145\n",
      "Train Epoch: 40 [20480/60000 (34%)]\tLoss: 0.384879\n",
      "Train Epoch: 40 [25600/60000 (42%)]\tLoss: 0.304100\n",
      "Train Epoch: 40 [30720/60000 (51%)]\tLoss: 0.374129\n",
      "Train Epoch: 40 [35840/60000 (59%)]\tLoss: 0.328391\n",
      "Train Epoch: 40 [40960/60000 (68%)]\tLoss: 0.378748\n",
      "Train Epoch: 40 [46080/60000 (76%)]\tLoss: 0.385304\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.347184\n",
      "Train Epoch: 40 [56320/60000 (93%)]\tLoss: 0.371332\n",
      "Test set: Avg. loss: 0.3971, Accuracy: 8581/10000 (85%)\n",
      "\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.347240\n",
      "Train Epoch: 41 [5120/60000 (8%)]\tLoss: 0.383537\n",
      "Train Epoch: 41 [10240/60000 (17%)]\tLoss: 0.400815\n",
      "Train Epoch: 41 [15360/60000 (25%)]\tLoss: 0.409189\n",
      "Train Epoch: 41 [20480/60000 (34%)]\tLoss: 0.344903\n",
      "Train Epoch: 41 [25600/60000 (42%)]\tLoss: 0.403755\n",
      "Train Epoch: 41 [30720/60000 (51%)]\tLoss: 0.343327\n",
      "Train Epoch: 41 [35840/60000 (59%)]\tLoss: 0.467073\n",
      "Train Epoch: 41 [40960/60000 (68%)]\tLoss: 0.409325\n",
      "Train Epoch: 41 [46080/60000 (76%)]\tLoss: 0.382539\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 0.465201\n",
      "Train Epoch: 41 [56320/60000 (93%)]\tLoss: 0.342803\n",
      "Test set: Avg. loss: 0.3946, Accuracy: 8600/10000 (86%)\n",
      "\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.393787\n",
      "Train Epoch: 42 [5120/60000 (8%)]\tLoss: 0.391099\n",
      "Train Epoch: 42 [10240/60000 (17%)]\tLoss: 0.350333\n",
      "Train Epoch: 42 [15360/60000 (25%)]\tLoss: 0.314474\n",
      "Train Epoch: 42 [20480/60000 (34%)]\tLoss: 0.381891\n",
      "Train Epoch: 42 [25600/60000 (42%)]\tLoss: 0.379635\n",
      "Train Epoch: 42 [30720/60000 (51%)]\tLoss: 0.367303\n",
      "Train Epoch: 42 [35840/60000 (59%)]\tLoss: 0.373330\n",
      "Train Epoch: 42 [40960/60000 (68%)]\tLoss: 0.342365\n",
      "Train Epoch: 42 [46080/60000 (76%)]\tLoss: 0.267741\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 0.381510\n",
      "Train Epoch: 42 [56320/60000 (93%)]\tLoss: 0.302823\n",
      "Test set: Avg. loss: 0.4073, Accuracy: 8528/10000 (85%)\n",
      "\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.363821\n",
      "Train Epoch: 43 [5120/60000 (8%)]\tLoss: 0.328263\n",
      "Train Epoch: 43 [10240/60000 (17%)]\tLoss: 0.395307\n",
      "Train Epoch: 43 [15360/60000 (25%)]\tLoss: 0.370173\n",
      "Train Epoch: 43 [20480/60000 (34%)]\tLoss: 0.387233\n",
      "Train Epoch: 43 [25600/60000 (42%)]\tLoss: 0.367076\n",
      "Train Epoch: 43 [30720/60000 (51%)]\tLoss: 0.389350\n",
      "Train Epoch: 43 [35840/60000 (59%)]\tLoss: 0.308395\n",
      "Train Epoch: 43 [40960/60000 (68%)]\tLoss: 0.333235\n",
      "Train Epoch: 43 [46080/60000 (76%)]\tLoss: 0.352512\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 0.371874\n",
      "Train Epoch: 43 [56320/60000 (93%)]\tLoss: 0.356130\n",
      "Test set: Avg. loss: 0.3958, Accuracy: 8565/10000 (85%)\n",
      "\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.299478\n",
      "Train Epoch: 44 [5120/60000 (8%)]\tLoss: 0.359366\n",
      "Train Epoch: 44 [10240/60000 (17%)]\tLoss: 0.340886\n",
      "Train Epoch: 44 [15360/60000 (25%)]\tLoss: 0.347676\n",
      "Train Epoch: 44 [20480/60000 (34%)]\tLoss: 0.319837\n",
      "Train Epoch: 44 [25600/60000 (42%)]\tLoss: 0.316394\n",
      "Train Epoch: 44 [30720/60000 (51%)]\tLoss: 0.320412\n",
      "Train Epoch: 44 [35840/60000 (59%)]\tLoss: 0.388522\n",
      "Train Epoch: 44 [40960/60000 (68%)]\tLoss: 0.345537\n",
      "Train Epoch: 44 [46080/60000 (76%)]\tLoss: 0.347870\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 0.324501\n",
      "Train Epoch: 44 [56320/60000 (93%)]\tLoss: 0.410719\n",
      "Test set: Avg. loss: 0.3885, Accuracy: 8627/10000 (86%)\n",
      "\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.378862\n",
      "Train Epoch: 45 [5120/60000 (8%)]\tLoss: 0.373823\n",
      "Train Epoch: 45 [10240/60000 (17%)]\tLoss: 0.320988\n",
      "Train Epoch: 45 [15360/60000 (25%)]\tLoss: 0.300574\n",
      "Train Epoch: 45 [20480/60000 (34%)]\tLoss: 0.330540\n",
      "Train Epoch: 45 [25600/60000 (42%)]\tLoss: 0.309317\n",
      "Train Epoch: 45 [30720/60000 (51%)]\tLoss: 0.368295\n",
      "Train Epoch: 45 [35840/60000 (59%)]\tLoss: 0.355906\n",
      "Train Epoch: 45 [40960/60000 (68%)]\tLoss: 0.353753\n",
      "Train Epoch: 45 [46080/60000 (76%)]\tLoss: 0.370433\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.357492\n",
      "Train Epoch: 45 [56320/60000 (93%)]\tLoss: 0.409582\n",
      "Test set: Avg. loss: 0.3941, Accuracy: 8589/10000 (85%)\n",
      "\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.345271\n",
      "Train Epoch: 46 [5120/60000 (8%)]\tLoss: 0.307007\n",
      "Train Epoch: 46 [10240/60000 (17%)]\tLoss: 0.318772\n",
      "Train Epoch: 46 [15360/60000 (25%)]\tLoss: 0.355468\n",
      "Train Epoch: 46 [20480/60000 (34%)]\tLoss: 0.396616\n",
      "Train Epoch: 46 [25600/60000 (42%)]\tLoss: 0.393128\n",
      "Train Epoch: 46 [30720/60000 (51%)]\tLoss: 0.278832\n",
      "Train Epoch: 46 [35840/60000 (59%)]\tLoss: 0.370195\n",
      "Train Epoch: 46 [40960/60000 (68%)]\tLoss: 0.297471\n",
      "Train Epoch: 46 [46080/60000 (76%)]\tLoss: 0.370545\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 0.330412\n",
      "Train Epoch: 46 [56320/60000 (93%)]\tLoss: 0.352842\n",
      "Test set: Avg. loss: 0.3864, Accuracy: 8621/10000 (86%)\n",
      "\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.379181\n",
      "Train Epoch: 47 [5120/60000 (8%)]\tLoss: 0.272942\n",
      "Train Epoch: 47 [10240/60000 (17%)]\tLoss: 0.384031\n",
      "Train Epoch: 47 [15360/60000 (25%)]\tLoss: 0.307815\n",
      "Train Epoch: 47 [20480/60000 (34%)]\tLoss: 0.314709\n",
      "Train Epoch: 47 [25600/60000 (42%)]\tLoss: 0.408019\n",
      "Train Epoch: 47 [30720/60000 (51%)]\tLoss: 0.357109\n",
      "Train Epoch: 47 [35840/60000 (59%)]\tLoss: 0.361312\n",
      "Train Epoch: 47 [40960/60000 (68%)]\tLoss: 0.361551\n",
      "Train Epoch: 47 [46080/60000 (76%)]\tLoss: 0.364494\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 0.339273\n",
      "Train Epoch: 47 [56320/60000 (93%)]\tLoss: 0.337784\n",
      "Test set: Avg. loss: 0.3868, Accuracy: 8624/10000 (86%)\n",
      "\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.401765\n",
      "Train Epoch: 48 [5120/60000 (8%)]\tLoss: 0.398985\n",
      "Train Epoch: 48 [10240/60000 (17%)]\tLoss: 0.303251\n",
      "Train Epoch: 48 [15360/60000 (25%)]\tLoss: 0.348700\n",
      "Train Epoch: 48 [20480/60000 (34%)]\tLoss: 0.334712\n",
      "Train Epoch: 48 [25600/60000 (42%)]\tLoss: 0.377284\n",
      "Train Epoch: 48 [30720/60000 (51%)]\tLoss: 0.348353\n",
      "Train Epoch: 48 [35840/60000 (59%)]\tLoss: 0.346744\n",
      "Train Epoch: 48 [40960/60000 (68%)]\tLoss: 0.395975\n",
      "Train Epoch: 48 [46080/60000 (76%)]\tLoss: 0.331005\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 0.330495\n",
      "Train Epoch: 48 [56320/60000 (93%)]\tLoss: 0.333538\n",
      "Test set: Avg. loss: 0.3965, Accuracy: 8587/10000 (85%)\n",
      "\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.353314\n",
      "Train Epoch: 49 [5120/60000 (8%)]\tLoss: 0.345323\n",
      "Train Epoch: 49 [10240/60000 (17%)]\tLoss: 0.331421\n",
      "Train Epoch: 49 [15360/60000 (25%)]\tLoss: 0.342956\n",
      "Train Epoch: 49 [20480/60000 (34%)]\tLoss: 0.296993\n",
      "Train Epoch: 49 [25600/60000 (42%)]\tLoss: 0.303784\n",
      "Train Epoch: 49 [30720/60000 (51%)]\tLoss: 0.334809\n",
      "Train Epoch: 49 [35840/60000 (59%)]\tLoss: 0.365974\n",
      "Train Epoch: 49 [40960/60000 (68%)]\tLoss: 0.354269\n",
      "Train Epoch: 49 [46080/60000 (76%)]\tLoss: 0.339654\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 0.320744\n",
      "Train Epoch: 49 [56320/60000 (93%)]\tLoss: 0.357065\n",
      "Test set: Avg. loss: 0.3817, Accuracy: 8648/10000 (86%)\n",
      "\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.299100\n",
      "Train Epoch: 50 [5120/60000 (8%)]\tLoss: 0.404885\n",
      "Train Epoch: 50 [10240/60000 (17%)]\tLoss: 0.329822\n",
      "Train Epoch: 50 [15360/60000 (25%)]\tLoss: 0.295659\n",
      "Train Epoch: 50 [20480/60000 (34%)]\tLoss: 0.324831\n",
      "Train Epoch: 50 [25600/60000 (42%)]\tLoss: 0.308231\n",
      "Train Epoch: 50 [30720/60000 (51%)]\tLoss: 0.346801\n",
      "Train Epoch: 50 [35840/60000 (59%)]\tLoss: 0.347664\n",
      "Train Epoch: 50 [40960/60000 (68%)]\tLoss: 0.348330\n",
      "Train Epoch: 50 [46080/60000 (76%)]\tLoss: 0.309837\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.388328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50 [56320/60000 (93%)]\tLoss: 0.358175\n",
      "Test set: Avg. loss: 0.3818, Accuracy: 8642/10000 (86%)\n",
      "\n",
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: 0.332626\n",
      "Train Epoch: 51 [5120/60000 (8%)]\tLoss: 0.306531\n",
      "Train Epoch: 51 [10240/60000 (17%)]\tLoss: 0.299271\n",
      "Train Epoch: 51 [15360/60000 (25%)]\tLoss: 0.319504\n",
      "Train Epoch: 51 [20480/60000 (34%)]\tLoss: 0.335327\n",
      "Train Epoch: 51 [25600/60000 (42%)]\tLoss: 0.359874\n",
      "Train Epoch: 51 [30720/60000 (51%)]\tLoss: 0.321603\n",
      "Train Epoch: 51 [35840/60000 (59%)]\tLoss: 0.268764\n",
      "Train Epoch: 51 [40960/60000 (68%)]\tLoss: 0.340302\n",
      "Train Epoch: 51 [46080/60000 (76%)]\tLoss: 0.322024\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 0.340765\n",
      "Train Epoch: 51 [56320/60000 (93%)]\tLoss: 0.370457\n",
      "Test set: Avg. loss: 0.3824, Accuracy: 8628/10000 (86%)\n",
      "\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: 0.295204\n",
      "Train Epoch: 52 [5120/60000 (8%)]\tLoss: 0.373785\n",
      "Train Epoch: 52 [10240/60000 (17%)]\tLoss: 0.315109\n",
      "Train Epoch: 52 [15360/60000 (25%)]\tLoss: 0.303469\n",
      "Train Epoch: 52 [20480/60000 (34%)]\tLoss: 0.365583\n",
      "Train Epoch: 52 [25600/60000 (42%)]\tLoss: 0.374023\n",
      "Train Epoch: 52 [30720/60000 (51%)]\tLoss: 0.353974\n",
      "Train Epoch: 52 [35840/60000 (59%)]\tLoss: 0.328149\n",
      "Train Epoch: 52 [40960/60000 (68%)]\tLoss: 0.308782\n",
      "Train Epoch: 52 [46080/60000 (76%)]\tLoss: 0.329838\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 0.330829\n",
      "Train Epoch: 52 [56320/60000 (93%)]\tLoss: 0.347854\n",
      "Test set: Avg. loss: 0.3783, Accuracy: 8657/10000 (86%)\n",
      "\n",
      "Train Epoch: 53 [0/60000 (0%)]\tLoss: 0.383576\n",
      "Train Epoch: 53 [5120/60000 (8%)]\tLoss: 0.344025\n",
      "Train Epoch: 53 [10240/60000 (17%)]\tLoss: 0.316691\n",
      "Train Epoch: 53 [15360/60000 (25%)]\tLoss: 0.352678\n",
      "Train Epoch: 53 [20480/60000 (34%)]\tLoss: 0.332810\n",
      "Train Epoch: 53 [25600/60000 (42%)]\tLoss: 0.358914\n",
      "Train Epoch: 53 [30720/60000 (51%)]\tLoss: 0.311290\n",
      "Train Epoch: 53 [35840/60000 (59%)]\tLoss: 0.285995\n",
      "Train Epoch: 53 [40960/60000 (68%)]\tLoss: 0.366743\n",
      "Train Epoch: 53 [46080/60000 (76%)]\tLoss: 0.384436\n",
      "Train Epoch: 53 [51200/60000 (85%)]\tLoss: 0.324003\n",
      "Train Epoch: 53 [56320/60000 (93%)]\tLoss: 0.364082\n",
      "Test set: Avg. loss: 0.3760, Accuracy: 8664/10000 (86%)\n",
      "\n",
      "Train Epoch: 54 [0/60000 (0%)]\tLoss: 0.341745\n",
      "Train Epoch: 54 [5120/60000 (8%)]\tLoss: 0.322051\n",
      "Train Epoch: 54 [10240/60000 (17%)]\tLoss: 0.352453\n",
      "Train Epoch: 54 [15360/60000 (25%)]\tLoss: 0.340904\n",
      "Train Epoch: 54 [20480/60000 (34%)]\tLoss: 0.297872\n",
      "Train Epoch: 54 [25600/60000 (42%)]\tLoss: 0.328327\n",
      "Train Epoch: 54 [30720/60000 (51%)]\tLoss: 0.359934\n",
      "Train Epoch: 54 [35840/60000 (59%)]\tLoss: 0.371344\n",
      "Train Epoch: 54 [40960/60000 (68%)]\tLoss: 0.439130\n",
      "Train Epoch: 54 [46080/60000 (76%)]\tLoss: 0.294589\n",
      "Train Epoch: 54 [51200/60000 (85%)]\tLoss: 0.319374\n",
      "Train Epoch: 54 [56320/60000 (93%)]\tLoss: 0.383020\n",
      "Test set: Avg. loss: 0.3762, Accuracy: 8658/10000 (86%)\n",
      "\n",
      "Train Epoch: 55 [0/60000 (0%)]\tLoss: 0.342056\n",
      "Train Epoch: 55 [5120/60000 (8%)]\tLoss: 0.337805\n",
      "Train Epoch: 55 [10240/60000 (17%)]\tLoss: 0.310317\n",
      "Train Epoch: 55 [15360/60000 (25%)]\tLoss: 0.342801\n",
      "Train Epoch: 55 [20480/60000 (34%)]\tLoss: 0.303184\n",
      "Train Epoch: 55 [25600/60000 (42%)]\tLoss: 0.304258\n",
      "Train Epoch: 55 [30720/60000 (51%)]\tLoss: 0.329511\n",
      "Train Epoch: 55 [35840/60000 (59%)]\tLoss: 0.310489\n",
      "Train Epoch: 55 [40960/60000 (68%)]\tLoss: 0.339578\n",
      "Train Epoch: 55 [46080/60000 (76%)]\tLoss: 0.316821\n",
      "Train Epoch: 55 [51200/60000 (85%)]\tLoss: 0.335589\n",
      "Train Epoch: 55 [56320/60000 (93%)]\tLoss: 0.321624\n",
      "Test set: Avg. loss: 0.3775, Accuracy: 8638/10000 (86%)\n",
      "\n",
      "Train Epoch: 56 [0/60000 (0%)]\tLoss: 0.289046\n",
      "Train Epoch: 56 [5120/60000 (8%)]\tLoss: 0.302583\n",
      "Train Epoch: 56 [10240/60000 (17%)]\tLoss: 0.303898\n",
      "Train Epoch: 56 [15360/60000 (25%)]\tLoss: 0.261160\n",
      "Train Epoch: 56 [20480/60000 (34%)]\tLoss: 0.327637\n",
      "Train Epoch: 56 [25600/60000 (42%)]\tLoss: 0.382149\n",
      "Train Epoch: 56 [30720/60000 (51%)]\tLoss: 0.320910\n",
      "Train Epoch: 56 [35840/60000 (59%)]\tLoss: 0.301649\n",
      "Train Epoch: 56 [40960/60000 (68%)]\tLoss: 0.357949\n",
      "Train Epoch: 56 [46080/60000 (76%)]\tLoss: 0.304674\n",
      "Train Epoch: 56 [51200/60000 (85%)]\tLoss: 0.356902\n",
      "Train Epoch: 56 [56320/60000 (93%)]\tLoss: 0.339689\n",
      "Test set: Avg. loss: 0.3762, Accuracy: 8664/10000 (86%)\n",
      "\n",
      "Train Epoch: 57 [0/60000 (0%)]\tLoss: 0.359877\n",
      "Train Epoch: 57 [5120/60000 (8%)]\tLoss: 0.324073\n",
      "Train Epoch: 57 [10240/60000 (17%)]\tLoss: 0.364090\n",
      "Train Epoch: 57 [15360/60000 (25%)]\tLoss: 0.337781\n",
      "Train Epoch: 57 [20480/60000 (34%)]\tLoss: 0.330412\n",
      "Train Epoch: 57 [25600/60000 (42%)]\tLoss: 0.292737\n",
      "Train Epoch: 57 [30720/60000 (51%)]\tLoss: 0.256435\n",
      "Train Epoch: 57 [35840/60000 (59%)]\tLoss: 0.318562\n",
      "Train Epoch: 57 [40960/60000 (68%)]\tLoss: 0.347375\n",
      "Train Epoch: 57 [46080/60000 (76%)]\tLoss: 0.359899\n",
      "Train Epoch: 57 [51200/60000 (85%)]\tLoss: 0.313909\n",
      "Train Epoch: 57 [56320/60000 (93%)]\tLoss: 0.383457\n",
      "Test set: Avg. loss: 0.3828, Accuracy: 8636/10000 (86%)\n",
      "\n",
      "Train Epoch: 58 [0/60000 (0%)]\tLoss: 0.313261\n",
      "Train Epoch: 58 [5120/60000 (8%)]\tLoss: 0.300972\n",
      "Train Epoch: 58 [10240/60000 (17%)]\tLoss: 0.292872\n",
      "Train Epoch: 58 [15360/60000 (25%)]\tLoss: 0.300861\n",
      "Train Epoch: 58 [20480/60000 (34%)]\tLoss: 0.311726\n",
      "Train Epoch: 58 [25600/60000 (42%)]\tLoss: 0.350244\n",
      "Train Epoch: 58 [30720/60000 (51%)]\tLoss: 0.297099\n",
      "Train Epoch: 58 [35840/60000 (59%)]\tLoss: 0.297339\n",
      "Train Epoch: 58 [40960/60000 (68%)]\tLoss: 0.304103\n",
      "Train Epoch: 58 [46080/60000 (76%)]\tLoss: 0.391369\n",
      "Train Epoch: 58 [51200/60000 (85%)]\tLoss: 0.304824\n",
      "Train Epoch: 58 [56320/60000 (93%)]\tLoss: 0.365545\n",
      "Test set: Avg. loss: 0.3749, Accuracy: 8647/10000 (86%)\n",
      "\n",
      "Train Epoch: 59 [0/60000 (0%)]\tLoss: 0.325438\n",
      "Train Epoch: 59 [5120/60000 (8%)]\tLoss: 0.336356\n",
      "Train Epoch: 59 [10240/60000 (17%)]\tLoss: 0.293662\n",
      "Train Epoch: 59 [15360/60000 (25%)]\tLoss: 0.312149\n",
      "Train Epoch: 59 [20480/60000 (34%)]\tLoss: 0.331249\n",
      "Train Epoch: 59 [25600/60000 (42%)]\tLoss: 0.289650\n",
      "Train Epoch: 59 [30720/60000 (51%)]\tLoss: 0.327620\n",
      "Train Epoch: 59 [35840/60000 (59%)]\tLoss: 0.318901\n",
      "Train Epoch: 59 [40960/60000 (68%)]\tLoss: 0.324485\n",
      "Train Epoch: 59 [46080/60000 (76%)]\tLoss: 0.336696\n",
      "Train Epoch: 59 [51200/60000 (85%)]\tLoss: 0.317459\n",
      "Train Epoch: 59 [56320/60000 (93%)]\tLoss: 0.337137\n",
      "Test set: Avg. loss: 0.3786, Accuracy: 8650/10000 (86%)\n",
      "\n",
      "Train Epoch: 60 [0/60000 (0%)]\tLoss: 0.348224\n",
      "Train Epoch: 60 [5120/60000 (8%)]\tLoss: 0.348246\n",
      "Train Epoch: 60 [10240/60000 (17%)]\tLoss: 0.310400\n",
      "Train Epoch: 60 [15360/60000 (25%)]\tLoss: 0.300398\n",
      "Train Epoch: 60 [20480/60000 (34%)]\tLoss: 0.293650\n",
      "Train Epoch: 60 [25600/60000 (42%)]\tLoss: 0.341171\n",
      "Train Epoch: 60 [30720/60000 (51%)]\tLoss: 0.312455\n",
      "Train Epoch: 60 [35840/60000 (59%)]\tLoss: 0.326217\n",
      "Train Epoch: 60 [40960/60000 (68%)]\tLoss: 0.335055\n",
      "Train Epoch: 60 [46080/60000 (76%)]\tLoss: 0.309067\n",
      "Train Epoch: 60 [51200/60000 (85%)]\tLoss: 0.347531\n",
      "Train Epoch: 60 [56320/60000 (93%)]\tLoss: 0.300899\n",
      "Test set: Avg. loss: 0.3797, Accuracy: 8642/10000 (86%)\n",
      "\n",
      "Train Epoch: 61 [0/60000 (0%)]\tLoss: 0.318950\n",
      "Train Epoch: 61 [5120/60000 (8%)]\tLoss: 0.382771\n",
      "Train Epoch: 61 [10240/60000 (17%)]\tLoss: 0.345406\n",
      "Train Epoch: 61 [15360/60000 (25%)]\tLoss: 0.359824\n",
      "Train Epoch: 61 [20480/60000 (34%)]\tLoss: 0.350315\n",
      "Train Epoch: 61 [25600/60000 (42%)]\tLoss: 0.352834\n",
      "Train Epoch: 61 [30720/60000 (51%)]\tLoss: 0.350326\n",
      "Train Epoch: 61 [35840/60000 (59%)]\tLoss: 0.275263\n",
      "Train Epoch: 61 [40960/60000 (68%)]\tLoss: 0.268647\n",
      "Train Epoch: 61 [46080/60000 (76%)]\tLoss: 0.332687\n",
      "Train Epoch: 61 [51200/60000 (85%)]\tLoss: 0.309520\n",
      "Train Epoch: 61 [56320/60000 (93%)]\tLoss: 0.314818\n",
      "Test set: Avg. loss: 0.3699, Accuracy: 8677/10000 (86%)\n",
      "\n",
      "Train Epoch: 62 [0/60000 (0%)]\tLoss: 0.347988\n",
      "Train Epoch: 62 [5120/60000 (8%)]\tLoss: 0.279785\n",
      "Train Epoch: 62 [10240/60000 (17%)]\tLoss: 0.325781\n",
      "Train Epoch: 62 [15360/60000 (25%)]\tLoss: 0.319975\n",
      "Train Epoch: 62 [20480/60000 (34%)]\tLoss: 0.317870\n",
      "Train Epoch: 62 [25600/60000 (42%)]\tLoss: 0.348817\n",
      "Train Epoch: 62 [30720/60000 (51%)]\tLoss: 0.302364\n",
      "Train Epoch: 62 [35840/60000 (59%)]\tLoss: 0.324907\n",
      "Train Epoch: 62 [40960/60000 (68%)]\tLoss: 0.346245\n",
      "Train Epoch: 62 [46080/60000 (76%)]\tLoss: 0.265074\n",
      "Train Epoch: 62 [51200/60000 (85%)]\tLoss: 0.340953\n",
      "Train Epoch: 62 [56320/60000 (93%)]\tLoss: 0.332358\n",
      "Test set: Avg. loss: 0.3721, Accuracy: 8671/10000 (86%)\n",
      "\n",
      "Train Epoch: 63 [0/60000 (0%)]\tLoss: 0.341023\n",
      "Train Epoch: 63 [5120/60000 (8%)]\tLoss: 0.257970\n",
      "Train Epoch: 63 [10240/60000 (17%)]\tLoss: 0.297478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 63 [15360/60000 (25%)]\tLoss: 0.317428\n",
      "Train Epoch: 63 [20480/60000 (34%)]\tLoss: 0.308406\n",
      "Train Epoch: 63 [25600/60000 (42%)]\tLoss: 0.339424\n",
      "Train Epoch: 63 [30720/60000 (51%)]\tLoss: 0.402591\n",
      "Train Epoch: 63 [35840/60000 (59%)]\tLoss: 0.314178\n",
      "Train Epoch: 63 [40960/60000 (68%)]\tLoss: 0.295772\n",
      "Train Epoch: 63 [46080/60000 (76%)]\tLoss: 0.245385\n",
      "Train Epoch: 63 [51200/60000 (85%)]\tLoss: 0.306571\n",
      "Train Epoch: 63 [56320/60000 (93%)]\tLoss: 0.312236\n",
      "Test set: Avg. loss: 0.3744, Accuracy: 8648/10000 (86%)\n",
      "\n",
      "Train Epoch: 64 [0/60000 (0%)]\tLoss: 0.267829\n",
      "Train Epoch: 64 [5120/60000 (8%)]\tLoss: 0.306347\n",
      "Train Epoch: 64 [10240/60000 (17%)]\tLoss: 0.274758\n",
      "Train Epoch: 64 [15360/60000 (25%)]\tLoss: 0.299466\n",
      "Train Epoch: 64 [20480/60000 (34%)]\tLoss: 0.295073\n",
      "Train Epoch: 64 [25600/60000 (42%)]\tLoss: 0.317330\n",
      "Train Epoch: 64 [30720/60000 (51%)]\tLoss: 0.294138\n",
      "Train Epoch: 64 [35840/60000 (59%)]\tLoss: 0.334834\n",
      "Train Epoch: 64 [40960/60000 (68%)]\tLoss: 0.280284\n",
      "Train Epoch: 64 [46080/60000 (76%)]\tLoss: 0.296633\n",
      "Train Epoch: 64 [51200/60000 (85%)]\tLoss: 0.301635\n",
      "Train Epoch: 64 [56320/60000 (93%)]\tLoss: 0.296876\n",
      "Test set: Avg. loss: 0.3637, Accuracy: 8697/10000 (86%)\n",
      "\n",
      "Train Epoch: 65 [0/60000 (0%)]\tLoss: 0.321312\n",
      "Train Epoch: 65 [5120/60000 (8%)]\tLoss: 0.303140\n",
      "Train Epoch: 65 [10240/60000 (17%)]\tLoss: 0.272274\n",
      "Train Epoch: 65 [15360/60000 (25%)]\tLoss: 0.313533\n",
      "Train Epoch: 65 [20480/60000 (34%)]\tLoss: 0.326414\n",
      "Train Epoch: 65 [25600/60000 (42%)]\tLoss: 0.292488\n",
      "Train Epoch: 65 [30720/60000 (51%)]\tLoss: 0.350144\n",
      "Train Epoch: 65 [35840/60000 (59%)]\tLoss: 0.335347\n",
      "Train Epoch: 65 [40960/60000 (68%)]\tLoss: 0.296477\n",
      "Train Epoch: 65 [46080/60000 (76%)]\tLoss: 0.314721\n",
      "Train Epoch: 65 [51200/60000 (85%)]\tLoss: 0.264642\n",
      "Train Epoch: 65 [56320/60000 (93%)]\tLoss: 0.332890\n",
      "Test set: Avg. loss: 0.3678, Accuracy: 8682/10000 (86%)\n",
      "\n",
      "Train Epoch: 66 [0/60000 (0%)]\tLoss: 0.327615\n",
      "Train Epoch: 66 [5120/60000 (8%)]\tLoss: 0.324428\n",
      "Train Epoch: 66 [10240/60000 (17%)]\tLoss: 0.273868\n",
      "Train Epoch: 66 [15360/60000 (25%)]\tLoss: 0.287220\n",
      "Train Epoch: 66 [20480/60000 (34%)]\tLoss: 0.335001\n",
      "Train Epoch: 66 [25600/60000 (42%)]\tLoss: 0.293117\n",
      "Train Epoch: 66 [30720/60000 (51%)]\tLoss: 0.326128\n",
      "Train Epoch: 66 [35840/60000 (59%)]\tLoss: 0.293638\n",
      "Train Epoch: 66 [40960/60000 (68%)]\tLoss: 0.285282\n",
      "Train Epoch: 66 [46080/60000 (76%)]\tLoss: 0.300665\n",
      "Train Epoch: 66 [51200/60000 (85%)]\tLoss: 0.311786\n",
      "Train Epoch: 66 [56320/60000 (93%)]\tLoss: 0.373898\n",
      "Test set: Avg. loss: 0.3705, Accuracy: 8655/10000 (86%)\n",
      "\n",
      "Train Epoch: 67 [0/60000 (0%)]\tLoss: 0.292087\n",
      "Train Epoch: 67 [5120/60000 (8%)]\tLoss: 0.335646\n",
      "Train Epoch: 67 [10240/60000 (17%)]\tLoss: 0.269579\n",
      "Train Epoch: 67 [15360/60000 (25%)]\tLoss: 0.274490\n",
      "Train Epoch: 67 [20480/60000 (34%)]\tLoss: 0.306307\n",
      "Train Epoch: 67 [25600/60000 (42%)]\tLoss: 0.354371\n",
      "Train Epoch: 67 [30720/60000 (51%)]\tLoss: 0.305423\n",
      "Train Epoch: 67 [35840/60000 (59%)]\tLoss: 0.305222\n",
      "Train Epoch: 67 [40960/60000 (68%)]\tLoss: 0.309303\n",
      "Train Epoch: 67 [46080/60000 (76%)]\tLoss: 0.318036\n",
      "Train Epoch: 67 [51200/60000 (85%)]\tLoss: 0.337399\n",
      "Train Epoch: 67 [56320/60000 (93%)]\tLoss: 0.321342\n",
      "Test set: Avg. loss: 0.3759, Accuracy: 8655/10000 (86%)\n",
      "\n",
      "Train Epoch: 68 [0/60000 (0%)]\tLoss: 0.366971\n",
      "Train Epoch: 68 [5120/60000 (8%)]\tLoss: 0.366185\n",
      "Train Epoch: 68 [10240/60000 (17%)]\tLoss: 0.292120\n",
      "Train Epoch: 68 [15360/60000 (25%)]\tLoss: 0.263199\n",
      "Train Epoch: 68 [20480/60000 (34%)]\tLoss: 0.344224\n",
      "Train Epoch: 68 [25600/60000 (42%)]\tLoss: 0.329227\n",
      "Train Epoch: 68 [30720/60000 (51%)]\tLoss: 0.285918\n",
      "Train Epoch: 68 [35840/60000 (59%)]\tLoss: 0.342573\n",
      "Train Epoch: 68 [40960/60000 (68%)]\tLoss: 0.299265\n",
      "Train Epoch: 68 [46080/60000 (76%)]\tLoss: 0.289119\n",
      "Train Epoch: 68 [51200/60000 (85%)]\tLoss: 0.237896\n",
      "Train Epoch: 68 [56320/60000 (93%)]\tLoss: 0.324607\n",
      "Test set: Avg. loss: 0.3609, Accuracy: 8710/10000 (87%)\n",
      "\n",
      "Train Epoch: 69 [0/60000 (0%)]\tLoss: 0.329296\n",
      "Train Epoch: 69 [5120/60000 (8%)]\tLoss: 0.324963\n",
      "Train Epoch: 69 [10240/60000 (17%)]\tLoss: 0.322141\n",
      "Train Epoch: 69 [15360/60000 (25%)]\tLoss: 0.315831\n",
      "Train Epoch: 69 [20480/60000 (34%)]\tLoss: 0.275250\n",
      "Train Epoch: 69 [25600/60000 (42%)]\tLoss: 0.296843\n",
      "Train Epoch: 69 [30720/60000 (51%)]\tLoss: 0.300938\n",
      "Train Epoch: 69 [35840/60000 (59%)]\tLoss: 0.348615\n",
      "Train Epoch: 69 [40960/60000 (68%)]\tLoss: 0.288608\n",
      "Train Epoch: 69 [46080/60000 (76%)]\tLoss: 0.311773\n",
      "Train Epoch: 69 [51200/60000 (85%)]\tLoss: 0.308983\n",
      "Train Epoch: 69 [56320/60000 (93%)]\tLoss: 0.282619\n",
      "Test set: Avg. loss: 0.3650, Accuracy: 8696/10000 (86%)\n",
      "\n",
      "Train Epoch: 70 [0/60000 (0%)]\tLoss: 0.300354\n",
      "Train Epoch: 70 [5120/60000 (8%)]\tLoss: 0.307391\n",
      "Train Epoch: 70 [10240/60000 (17%)]\tLoss: 0.299504\n",
      "Train Epoch: 70 [15360/60000 (25%)]\tLoss: 0.324315\n",
      "Train Epoch: 70 [20480/60000 (34%)]\tLoss: 0.318622\n",
      "Train Epoch: 70 [25600/60000 (42%)]\tLoss: 0.324679\n",
      "Train Epoch: 70 [30720/60000 (51%)]\tLoss: 0.341986\n",
      "Train Epoch: 70 [35840/60000 (59%)]\tLoss: 0.241694\n",
      "Train Epoch: 70 [40960/60000 (68%)]\tLoss: 0.293233\n",
      "Train Epoch: 70 [46080/60000 (76%)]\tLoss: 0.263030\n",
      "Train Epoch: 70 [51200/60000 (85%)]\tLoss: 0.296242\n",
      "Train Epoch: 70 [56320/60000 (93%)]\tLoss: 0.326558\n",
      "Test set: Avg. loss: 0.3662, Accuracy: 8690/10000 (86%)\n",
      "\n",
      "Train Epoch: 71 [0/60000 (0%)]\tLoss: 0.296546\n",
      "Train Epoch: 71 [5120/60000 (8%)]\tLoss: 0.333783\n",
      "Train Epoch: 71 [10240/60000 (17%)]\tLoss: 0.337897\n",
      "Train Epoch: 71 [15360/60000 (25%)]\tLoss: 0.284500\n",
      "Train Epoch: 71 [20480/60000 (34%)]\tLoss: 0.351490\n",
      "Train Epoch: 71 [25600/60000 (42%)]\tLoss: 0.290884\n",
      "Train Epoch: 71 [30720/60000 (51%)]\tLoss: 0.288087\n",
      "Train Epoch: 71 [35840/60000 (59%)]\tLoss: 0.325416\n",
      "Train Epoch: 71 [40960/60000 (68%)]\tLoss: 0.302105\n",
      "Train Epoch: 71 [46080/60000 (76%)]\tLoss: 0.302656\n",
      "Train Epoch: 71 [51200/60000 (85%)]\tLoss: 0.301706\n",
      "Train Epoch: 71 [56320/60000 (93%)]\tLoss: 0.312151\n",
      "Test set: Avg. loss: 0.3603, Accuracy: 8692/10000 (86%)\n",
      "\n",
      "Train Epoch: 72 [0/60000 (0%)]\tLoss: 0.325549\n",
      "Train Epoch: 72 [5120/60000 (8%)]\tLoss: 0.331262\n",
      "Train Epoch: 72 [10240/60000 (17%)]\tLoss: 0.372139\n",
      "Train Epoch: 72 [15360/60000 (25%)]\tLoss: 0.297597\n",
      "Train Epoch: 72 [20480/60000 (34%)]\tLoss: 0.275736\n",
      "Train Epoch: 72 [25600/60000 (42%)]\tLoss: 0.274954\n",
      "Train Epoch: 72 [30720/60000 (51%)]\tLoss: 0.261986\n",
      "Train Epoch: 72 [35840/60000 (59%)]\tLoss: 0.356152\n",
      "Train Epoch: 72 [40960/60000 (68%)]\tLoss: 0.273773\n",
      "Train Epoch: 72 [46080/60000 (76%)]\tLoss: 0.291930\n",
      "Train Epoch: 72 [51200/60000 (85%)]\tLoss: 0.267764\n",
      "Train Epoch: 72 [56320/60000 (93%)]\tLoss: 0.276010\n",
      "Test set: Avg. loss: 0.3583, Accuracy: 8701/10000 (87%)\n",
      "\n",
      "Train Epoch: 73 [0/60000 (0%)]\tLoss: 0.340893\n",
      "Train Epoch: 73 [5120/60000 (8%)]\tLoss: 0.364116\n",
      "Train Epoch: 73 [10240/60000 (17%)]\tLoss: 0.307125\n",
      "Train Epoch: 73 [15360/60000 (25%)]\tLoss: 0.315233\n",
      "Train Epoch: 73 [20480/60000 (34%)]\tLoss: 0.313726\n",
      "Train Epoch: 73 [25600/60000 (42%)]\tLoss: 0.348409\n",
      "Train Epoch: 73 [30720/60000 (51%)]\tLoss: 0.284002\n",
      "Train Epoch: 73 [35840/60000 (59%)]\tLoss: 0.275571\n",
      "Train Epoch: 73 [40960/60000 (68%)]\tLoss: 0.264896\n",
      "Train Epoch: 73 [46080/60000 (76%)]\tLoss: 0.323032\n",
      "Train Epoch: 73 [51200/60000 (85%)]\tLoss: 0.340779\n",
      "Train Epoch: 73 [56320/60000 (93%)]\tLoss: 0.290722\n",
      "Test set: Avg. loss: 0.3572, Accuracy: 8710/10000 (87%)\n",
      "\n",
      "Train Epoch: 74 [0/60000 (0%)]\tLoss: 0.283813\n",
      "Train Epoch: 74 [5120/60000 (8%)]\tLoss: 0.330014\n",
      "Train Epoch: 74 [10240/60000 (17%)]\tLoss: 0.355374\n",
      "Train Epoch: 74 [15360/60000 (25%)]\tLoss: 0.298503\n",
      "Train Epoch: 74 [20480/60000 (34%)]\tLoss: 0.234612\n",
      "Train Epoch: 74 [25600/60000 (42%)]\tLoss: 0.327887\n",
      "Train Epoch: 74 [30720/60000 (51%)]\tLoss: 0.275907\n",
      "Train Epoch: 74 [35840/60000 (59%)]\tLoss: 0.336290\n",
      "Train Epoch: 74 [40960/60000 (68%)]\tLoss: 0.303593\n",
      "Train Epoch: 74 [46080/60000 (76%)]\tLoss: 0.313281\n",
      "Train Epoch: 74 [51200/60000 (85%)]\tLoss: 0.320521\n",
      "Train Epoch: 74 [56320/60000 (93%)]\tLoss: 0.308188\n",
      "Test set: Avg. loss: 0.3579, Accuracy: 8719/10000 (87%)\n",
      "\n",
      "Train Epoch: 75 [0/60000 (0%)]\tLoss: 0.258832\n",
      "Train Epoch: 75 [5120/60000 (8%)]\tLoss: 0.300865\n",
      "Train Epoch: 75 [10240/60000 (17%)]\tLoss: 0.316685\n",
      "Train Epoch: 75 [15360/60000 (25%)]\tLoss: 0.275983\n",
      "Train Epoch: 75 [20480/60000 (34%)]\tLoss: 0.318822\n",
      "Train Epoch: 75 [25600/60000 (42%)]\tLoss: 0.290350\n",
      "Train Epoch: 75 [30720/60000 (51%)]\tLoss: 0.332044\n",
      "Train Epoch: 75 [35840/60000 (59%)]\tLoss: 0.281099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 75 [40960/60000 (68%)]\tLoss: 0.303404\n",
      "Train Epoch: 75 [46080/60000 (76%)]\tLoss: 0.333078\n",
      "Train Epoch: 75 [51200/60000 (85%)]\tLoss: 0.350449\n",
      "Train Epoch: 75 [56320/60000 (93%)]\tLoss: 0.254048\n",
      "Test set: Avg. loss: 0.3532, Accuracy: 8721/10000 (87%)\n",
      "\n",
      "Train Epoch: 76 [0/60000 (0%)]\tLoss: 0.369914\n",
      "Train Epoch: 76 [5120/60000 (8%)]\tLoss: 0.274867\n",
      "Train Epoch: 76 [10240/60000 (17%)]\tLoss: 0.280263\n",
      "Train Epoch: 76 [15360/60000 (25%)]\tLoss: 0.281796\n",
      "Train Epoch: 76 [20480/60000 (34%)]\tLoss: 0.309889\n",
      "Train Epoch: 76 [25600/60000 (42%)]\tLoss: 0.297363\n",
      "Train Epoch: 76 [30720/60000 (51%)]\tLoss: 0.255382\n",
      "Train Epoch: 76 [35840/60000 (59%)]\tLoss: 0.270031\n",
      "Train Epoch: 76 [40960/60000 (68%)]\tLoss: 0.333285\n",
      "Train Epoch: 76 [46080/60000 (76%)]\tLoss: 0.310248\n",
      "Train Epoch: 76 [51200/60000 (85%)]\tLoss: 0.294330\n",
      "Train Epoch: 76 [56320/60000 (93%)]\tLoss: 0.259288\n",
      "Test set: Avg. loss: 0.3557, Accuracy: 8717/10000 (87%)\n",
      "\n",
      "Train Epoch: 77 [0/60000 (0%)]\tLoss: 0.340424\n",
      "Train Epoch: 77 [5120/60000 (8%)]\tLoss: 0.282114\n",
      "Train Epoch: 77 [10240/60000 (17%)]\tLoss: 0.206936\n",
      "Train Epoch: 77 [15360/60000 (25%)]\tLoss: 0.271634\n",
      "Train Epoch: 77 [20480/60000 (34%)]\tLoss: 0.273914\n",
      "Train Epoch: 77 [25600/60000 (42%)]\tLoss: 0.284051\n",
      "Train Epoch: 77 [30720/60000 (51%)]\tLoss: 0.263618\n",
      "Train Epoch: 77 [35840/60000 (59%)]\tLoss: 0.272949\n",
      "Train Epoch: 77 [40960/60000 (68%)]\tLoss: 0.261991\n",
      "Train Epoch: 77 [46080/60000 (76%)]\tLoss: 0.317895\n",
      "Train Epoch: 77 [51200/60000 (85%)]\tLoss: 0.306243\n",
      "Train Epoch: 77 [56320/60000 (93%)]\tLoss: 0.315039\n",
      "Test set: Avg. loss: 0.3546, Accuracy: 8718/10000 (87%)\n",
      "\n",
      "Train Epoch: 78 [0/60000 (0%)]\tLoss: 0.360019\n",
      "Train Epoch: 78 [5120/60000 (8%)]\tLoss: 0.240427\n",
      "Train Epoch: 78 [10240/60000 (17%)]\tLoss: 0.294973\n",
      "Train Epoch: 78 [15360/60000 (25%)]\tLoss: 0.244792\n",
      "Train Epoch: 78 [20480/60000 (34%)]\tLoss: 0.298662\n",
      "Train Epoch: 78 [25600/60000 (42%)]\tLoss: 0.289547\n",
      "Train Epoch: 78 [30720/60000 (51%)]\tLoss: 0.339337\n",
      "Train Epoch: 78 [35840/60000 (59%)]\tLoss: 0.320201\n",
      "Train Epoch: 78 [40960/60000 (68%)]\tLoss: 0.303627\n",
      "Train Epoch: 78 [46080/60000 (76%)]\tLoss: 0.256852\n",
      "Train Epoch: 78 [51200/60000 (85%)]\tLoss: 0.242444\n",
      "Train Epoch: 78 [56320/60000 (93%)]\tLoss: 0.272424\n",
      "Test set: Avg. loss: 0.3568, Accuracy: 8716/10000 (87%)\n",
      "\n",
      "Train Epoch: 79 [0/60000 (0%)]\tLoss: 0.304637\n",
      "Train Epoch: 79 [5120/60000 (8%)]\tLoss: 0.267382\n",
      "Train Epoch: 79 [10240/60000 (17%)]\tLoss: 0.281116\n",
      "Train Epoch: 79 [15360/60000 (25%)]\tLoss: 0.334288\n",
      "Train Epoch: 79 [20480/60000 (34%)]\tLoss: 0.317626\n",
      "Train Epoch: 79 [25600/60000 (42%)]\tLoss: 0.249086\n",
      "Train Epoch: 79 [30720/60000 (51%)]\tLoss: 0.295675\n",
      "Train Epoch: 79 [35840/60000 (59%)]\tLoss: 0.253517\n",
      "Train Epoch: 79 [40960/60000 (68%)]\tLoss: 0.295829\n",
      "Train Epoch: 79 [46080/60000 (76%)]\tLoss: 0.256498\n",
      "Train Epoch: 79 [51200/60000 (85%)]\tLoss: 0.328570\n",
      "Train Epoch: 79 [56320/60000 (93%)]\tLoss: 0.318243\n",
      "Test set: Avg. loss: 0.3546, Accuracy: 8722/10000 (87%)\n",
      "\n",
      "Train Epoch: 80 [0/60000 (0%)]\tLoss: 0.309685\n",
      "Train Epoch: 80 [5120/60000 (8%)]\tLoss: 0.283629\n",
      "Train Epoch: 80 [10240/60000 (17%)]\tLoss: 0.308978\n",
      "Train Epoch: 80 [15360/60000 (25%)]\tLoss: 0.267522\n",
      "Train Epoch: 80 [20480/60000 (34%)]\tLoss: 0.320061\n",
      "Train Epoch: 80 [25600/60000 (42%)]\tLoss: 0.306865\n",
      "Train Epoch: 80 [30720/60000 (51%)]\tLoss: 0.283028\n",
      "Train Epoch: 80 [35840/60000 (59%)]\tLoss: 0.322572\n",
      "Train Epoch: 80 [40960/60000 (68%)]\tLoss: 0.266749\n",
      "Train Epoch: 80 [46080/60000 (76%)]\tLoss: 0.276320\n",
      "Train Epoch: 80 [51200/60000 (85%)]\tLoss: 0.262455\n",
      "Train Epoch: 80 [56320/60000 (93%)]\tLoss: 0.243574\n",
      "Test set: Avg. loss: 0.3546, Accuracy: 8717/10000 (87%)\n",
      "\n",
      "Train Epoch: 81 [0/60000 (0%)]\tLoss: 0.275101\n",
      "Train Epoch: 81 [5120/60000 (8%)]\tLoss: 0.305375\n",
      "Train Epoch: 81 [10240/60000 (17%)]\tLoss: 0.257978\n",
      "Train Epoch: 81 [15360/60000 (25%)]\tLoss: 0.301165\n",
      "Train Epoch: 81 [20480/60000 (34%)]\tLoss: 0.342336\n",
      "Train Epoch: 81 [25600/60000 (42%)]\tLoss: 0.244628\n",
      "Train Epoch: 81 [30720/60000 (51%)]\tLoss: 0.315362\n",
      "Train Epoch: 81 [35840/60000 (59%)]\tLoss: 0.276153\n",
      "Train Epoch: 81 [40960/60000 (68%)]\tLoss: 0.286147\n",
      "Train Epoch: 81 [46080/60000 (76%)]\tLoss: 0.278654\n",
      "Train Epoch: 81 [51200/60000 (85%)]\tLoss: 0.294727\n",
      "Train Epoch: 81 [56320/60000 (93%)]\tLoss: 0.393211\n",
      "Test set: Avg. loss: 0.3559, Accuracy: 8717/10000 (87%)\n",
      "\n",
      "Train Epoch: 82 [0/60000 (0%)]\tLoss: 0.302495\n",
      "Train Epoch: 82 [5120/60000 (8%)]\tLoss: 0.250028\n",
      "Train Epoch: 82 [10240/60000 (17%)]\tLoss: 0.232649\n",
      "Train Epoch: 82 [15360/60000 (25%)]\tLoss: 0.271501\n",
      "Train Epoch: 82 [20480/60000 (34%)]\tLoss: 0.269796\n",
      "Train Epoch: 82 [25600/60000 (42%)]\tLoss: 0.278064\n",
      "Train Epoch: 82 [30720/60000 (51%)]\tLoss: 0.325817\n",
      "Train Epoch: 82 [35840/60000 (59%)]\tLoss: 0.260923\n",
      "Train Epoch: 82 [40960/60000 (68%)]\tLoss: 0.257149\n",
      "Train Epoch: 82 [46080/60000 (76%)]\tLoss: 0.261878\n",
      "Train Epoch: 82 [51200/60000 (85%)]\tLoss: 0.255516\n",
      "Train Epoch: 82 [56320/60000 (93%)]\tLoss: 0.351907\n",
      "Test set: Avg. loss: 0.3613, Accuracy: 8692/10000 (86%)\n",
      "\n",
      "Train Epoch: 83 [0/60000 (0%)]\tLoss: 0.270352\n",
      "Train Epoch: 83 [5120/60000 (8%)]\tLoss: 0.318111\n",
      "Train Epoch: 83 [10240/60000 (17%)]\tLoss: 0.258593\n",
      "Train Epoch: 83 [15360/60000 (25%)]\tLoss: 0.264874\n",
      "Train Epoch: 83 [20480/60000 (34%)]\tLoss: 0.251764\n",
      "Train Epoch: 83 [25600/60000 (42%)]\tLoss: 0.264962\n",
      "Train Epoch: 83 [30720/60000 (51%)]\tLoss: 0.319173\n",
      "Train Epoch: 83 [35840/60000 (59%)]\tLoss: 0.326957\n",
      "Train Epoch: 83 [40960/60000 (68%)]\tLoss: 0.280472\n",
      "Train Epoch: 83 [46080/60000 (76%)]\tLoss: 0.298293\n",
      "Train Epoch: 83 [51200/60000 (85%)]\tLoss: 0.264464\n",
      "Train Epoch: 83 [56320/60000 (93%)]\tLoss: 0.293429\n",
      "Test set: Avg. loss: 0.3492, Accuracy: 8733/10000 (87%)\n",
      "\n",
      "Train Epoch: 84 [0/60000 (0%)]\tLoss: 0.300161\n",
      "Train Epoch: 84 [5120/60000 (8%)]\tLoss: 0.242242\n",
      "Train Epoch: 84 [10240/60000 (17%)]\tLoss: 0.286136\n",
      "Train Epoch: 84 [15360/60000 (25%)]\tLoss: 0.367962\n",
      "Train Epoch: 84 [20480/60000 (34%)]\tLoss: 0.310418\n",
      "Train Epoch: 84 [25600/60000 (42%)]\tLoss: 0.319861\n",
      "Train Epoch: 84 [30720/60000 (51%)]\tLoss: 0.265290\n",
      "Train Epoch: 84 [35840/60000 (59%)]\tLoss: 0.285628\n",
      "Train Epoch: 84 [40960/60000 (68%)]\tLoss: 0.322434\n",
      "Train Epoch: 84 [46080/60000 (76%)]\tLoss: 0.241977\n",
      "Train Epoch: 84 [51200/60000 (85%)]\tLoss: 0.321221\n",
      "Train Epoch: 84 [56320/60000 (93%)]\tLoss: 0.283042\n",
      "Test set: Avg. loss: 0.3601, Accuracy: 8697/10000 (86%)\n",
      "\n",
      "Train Epoch: 85 [0/60000 (0%)]\tLoss: 0.249873\n",
      "Train Epoch: 85 [5120/60000 (8%)]\tLoss: 0.306103\n",
      "Train Epoch: 85 [10240/60000 (17%)]\tLoss: 0.255091\n",
      "Train Epoch: 85 [15360/60000 (25%)]\tLoss: 0.303234\n",
      "Train Epoch: 85 [20480/60000 (34%)]\tLoss: 0.282772\n",
      "Train Epoch: 85 [25600/60000 (42%)]\tLoss: 0.264285\n",
      "Train Epoch: 85 [30720/60000 (51%)]\tLoss: 0.296580\n",
      "Train Epoch: 85 [35840/60000 (59%)]\tLoss: 0.268483\n",
      "Train Epoch: 85 [40960/60000 (68%)]\tLoss: 0.254895\n",
      "Train Epoch: 85 [46080/60000 (76%)]\tLoss: 0.298906\n",
      "Train Epoch: 85 [51200/60000 (85%)]\tLoss: 0.282148\n",
      "Train Epoch: 85 [56320/60000 (93%)]\tLoss: 0.261204\n",
      "Test set: Avg. loss: 0.3489, Accuracy: 8755/10000 (87%)\n",
      "\n",
      "Train Epoch: 86 [0/60000 (0%)]\tLoss: 0.286766\n",
      "Train Epoch: 86 [5120/60000 (8%)]\tLoss: 0.249380\n",
      "Train Epoch: 86 [10240/60000 (17%)]\tLoss: 0.293957\n",
      "Train Epoch: 86 [15360/60000 (25%)]\tLoss: 0.278989\n",
      "Train Epoch: 86 [20480/60000 (34%)]\tLoss: 0.309313\n",
      "Train Epoch: 86 [25600/60000 (42%)]\tLoss: 0.351832\n",
      "Train Epoch: 86 [30720/60000 (51%)]\tLoss: 0.253491\n",
      "Train Epoch: 86 [35840/60000 (59%)]\tLoss: 0.239750\n",
      "Train Epoch: 86 [40960/60000 (68%)]\tLoss: 0.307219\n",
      "Train Epoch: 86 [46080/60000 (76%)]\tLoss: 0.269317\n",
      "Train Epoch: 86 [51200/60000 (85%)]\tLoss: 0.349476\n",
      "Train Epoch: 86 [56320/60000 (93%)]\tLoss: 0.340870\n",
      "Test set: Avg. loss: 0.3501, Accuracy: 8742/10000 (87%)\n",
      "\n",
      "Train Epoch: 87 [0/60000 (0%)]\tLoss: 0.257969\n",
      "Train Epoch: 87 [5120/60000 (8%)]\tLoss: 0.249611\n",
      "Train Epoch: 87 [10240/60000 (17%)]\tLoss: 0.324441\n",
      "Train Epoch: 87 [15360/60000 (25%)]\tLoss: 0.308174\n",
      "Train Epoch: 87 [20480/60000 (34%)]\tLoss: 0.293647\n",
      "Train Epoch: 87 [25600/60000 (42%)]\tLoss: 0.315204\n",
      "Train Epoch: 87 [30720/60000 (51%)]\tLoss: 0.289750\n",
      "Train Epoch: 87 [35840/60000 (59%)]\tLoss: 0.316361\n",
      "Train Epoch: 87 [40960/60000 (68%)]\tLoss: 0.249853\n",
      "Train Epoch: 87 [46080/60000 (76%)]\tLoss: 0.267721\n",
      "Train Epoch: 87 [51200/60000 (85%)]\tLoss: 0.313921\n",
      "Train Epoch: 87 [56320/60000 (93%)]\tLoss: 0.301668\n",
      "Test set: Avg. loss: 0.3436, Accuracy: 8783/10000 (87%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 88 [0/60000 (0%)]\tLoss: 0.283132\n",
      "Train Epoch: 88 [5120/60000 (8%)]\tLoss: 0.302022\n",
      "Train Epoch: 88 [10240/60000 (17%)]\tLoss: 0.267426\n",
      "Train Epoch: 88 [15360/60000 (25%)]\tLoss: 0.299103\n",
      "Train Epoch: 88 [20480/60000 (34%)]\tLoss: 0.288793\n",
      "Train Epoch: 88 [25600/60000 (42%)]\tLoss: 0.326746\n",
      "Train Epoch: 88 [30720/60000 (51%)]\tLoss: 0.287901\n",
      "Train Epoch: 88 [35840/60000 (59%)]\tLoss: 0.290086\n",
      "Train Epoch: 88 [40960/60000 (68%)]\tLoss: 0.246011\n",
      "Train Epoch: 88 [46080/60000 (76%)]\tLoss: 0.272172\n",
      "Train Epoch: 88 [51200/60000 (85%)]\tLoss: 0.272394\n",
      "Train Epoch: 88 [56320/60000 (93%)]\tLoss: 0.264174\n",
      "Test set: Avg. loss: 0.3600, Accuracy: 8711/10000 (87%)\n",
      "\n",
      "Train Epoch: 89 [0/60000 (0%)]\tLoss: 0.240338\n",
      "Train Epoch: 89 [5120/60000 (8%)]\tLoss: 0.286506\n",
      "Train Epoch: 89 [10240/60000 (17%)]\tLoss: 0.286725\n",
      "Train Epoch: 89 [15360/60000 (25%)]\tLoss: 0.292430\n",
      "Train Epoch: 89 [20480/60000 (34%)]\tLoss: 0.284830\n",
      "Train Epoch: 89 [25600/60000 (42%)]\tLoss: 0.293046\n",
      "Train Epoch: 89 [30720/60000 (51%)]\tLoss: 0.317369\n",
      "Train Epoch: 89 [35840/60000 (59%)]\tLoss: 0.257968\n",
      "Train Epoch: 89 [40960/60000 (68%)]\tLoss: 0.232448\n",
      "Train Epoch: 89 [46080/60000 (76%)]\tLoss: 0.233178\n",
      "Train Epoch: 89 [51200/60000 (85%)]\tLoss: 0.255306\n",
      "Train Epoch: 89 [56320/60000 (93%)]\tLoss: 0.313862\n",
      "Test set: Avg. loss: 0.3463, Accuracy: 8769/10000 (87%)\n",
      "\n",
      "Train Epoch: 90 [0/60000 (0%)]\tLoss: 0.258703\n",
      "Train Epoch: 90 [5120/60000 (8%)]\tLoss: 0.305779\n",
      "Train Epoch: 90 [10240/60000 (17%)]\tLoss: 0.264735\n",
      "Train Epoch: 90 [15360/60000 (25%)]\tLoss: 0.301168\n",
      "Train Epoch: 90 [20480/60000 (34%)]\tLoss: 0.298747\n",
      "Train Epoch: 90 [25600/60000 (42%)]\tLoss: 0.321432\n",
      "Train Epoch: 90 [30720/60000 (51%)]\tLoss: 0.306124\n",
      "Train Epoch: 90 [35840/60000 (59%)]\tLoss: 0.243317\n",
      "Train Epoch: 90 [40960/60000 (68%)]\tLoss: 0.297759\n",
      "Train Epoch: 90 [46080/60000 (76%)]\tLoss: 0.350773\n",
      "Train Epoch: 90 [51200/60000 (85%)]\tLoss: 0.341668\n",
      "Train Epoch: 90 [56320/60000 (93%)]\tLoss: 0.282884\n",
      "Test set: Avg. loss: 0.3449, Accuracy: 8773/10000 (87%)\n",
      "\n",
      "Train Epoch: 91 [0/60000 (0%)]\tLoss: 0.249409\n",
      "Train Epoch: 91 [5120/60000 (8%)]\tLoss: 0.285128\n",
      "Train Epoch: 91 [10240/60000 (17%)]\tLoss: 0.234206\n",
      "Train Epoch: 91 [15360/60000 (25%)]\tLoss: 0.239465\n",
      "Train Epoch: 91 [20480/60000 (34%)]\tLoss: 0.297217\n",
      "Train Epoch: 91 [25600/60000 (42%)]\tLoss: 0.304073\n",
      "Train Epoch: 91 [30720/60000 (51%)]\tLoss: 0.260518\n",
      "Train Epoch: 91 [35840/60000 (59%)]\tLoss: 0.278962\n",
      "Train Epoch: 91 [40960/60000 (68%)]\tLoss: 0.270465\n",
      "Train Epoch: 91 [46080/60000 (76%)]\tLoss: 0.181237\n",
      "Train Epoch: 91 [51200/60000 (85%)]\tLoss: 0.281758\n",
      "Train Epoch: 91 [56320/60000 (93%)]\tLoss: 0.277245\n",
      "Test set: Avg. loss: 0.3443, Accuracy: 8773/10000 (87%)\n",
      "\n",
      "Train Epoch: 92 [0/60000 (0%)]\tLoss: 0.300056\n",
      "Train Epoch: 92 [5120/60000 (8%)]\tLoss: 0.299001\n",
      "Train Epoch: 92 [10240/60000 (17%)]\tLoss: 0.274200\n",
      "Train Epoch: 92 [15360/60000 (25%)]\tLoss: 0.281058\n",
      "Train Epoch: 92 [20480/60000 (34%)]\tLoss: 0.256837\n",
      "Train Epoch: 92 [25600/60000 (42%)]\tLoss: 0.269356\n",
      "Train Epoch: 92 [30720/60000 (51%)]\tLoss: 0.230958\n",
      "Train Epoch: 92 [35840/60000 (59%)]\tLoss: 0.281410\n",
      "Train Epoch: 92 [40960/60000 (68%)]\tLoss: 0.265697\n",
      "Train Epoch: 92 [46080/60000 (76%)]\tLoss: 0.228811\n",
      "Train Epoch: 92 [51200/60000 (85%)]\tLoss: 0.334227\n",
      "Train Epoch: 92 [56320/60000 (93%)]\tLoss: 0.318421\n",
      "Test set: Avg. loss: 0.3403, Accuracy: 8783/10000 (87%)\n",
      "\n",
      "Train Epoch: 93 [0/60000 (0%)]\tLoss: 0.284503\n",
      "Train Epoch: 93 [5120/60000 (8%)]\tLoss: 0.231679\n",
      "Train Epoch: 93 [10240/60000 (17%)]\tLoss: 0.294297\n",
      "Train Epoch: 93 [15360/60000 (25%)]\tLoss: 0.293447\n",
      "Train Epoch: 93 [20480/60000 (34%)]\tLoss: 0.258235\n",
      "Train Epoch: 93 [25600/60000 (42%)]\tLoss: 0.280437\n",
      "Train Epoch: 93 [30720/60000 (51%)]\tLoss: 0.291861\n",
      "Train Epoch: 93 [35840/60000 (59%)]\tLoss: 0.262535\n",
      "Train Epoch: 93 [40960/60000 (68%)]\tLoss: 0.294414\n",
      "Train Epoch: 93 [46080/60000 (76%)]\tLoss: 0.253760\n",
      "Train Epoch: 93 [51200/60000 (85%)]\tLoss: 0.281853\n",
      "Train Epoch: 93 [56320/60000 (93%)]\tLoss: 0.231858\n",
      "Test set: Avg. loss: 0.3435, Accuracy: 8767/10000 (87%)\n",
      "\n",
      "Train Epoch: 94 [0/60000 (0%)]\tLoss: 0.281511\n",
      "Train Epoch: 94 [5120/60000 (8%)]\tLoss: 0.258303\n",
      "Train Epoch: 94 [10240/60000 (17%)]\tLoss: 0.262285\n",
      "Train Epoch: 94 [15360/60000 (25%)]\tLoss: 0.338275\n",
      "Train Epoch: 94 [20480/60000 (34%)]\tLoss: 0.254935\n",
      "Train Epoch: 94 [25600/60000 (42%)]\tLoss: 0.348891\n",
      "Train Epoch: 94 [30720/60000 (51%)]\tLoss: 0.304121\n",
      "Train Epoch: 94 [35840/60000 (59%)]\tLoss: 0.289984\n",
      "Train Epoch: 94 [40960/60000 (68%)]\tLoss: 0.311291\n",
      "Train Epoch: 94 [46080/60000 (76%)]\tLoss: 0.286723\n",
      "Train Epoch: 94 [51200/60000 (85%)]\tLoss: 0.252683\n",
      "Train Epoch: 94 [56320/60000 (93%)]\tLoss: 0.257265\n",
      "Test set: Avg. loss: 0.3413, Accuracy: 8795/10000 (87%)\n",
      "\n",
      "Train Epoch: 95 [0/60000 (0%)]\tLoss: 0.265487\n",
      "Train Epoch: 95 [5120/60000 (8%)]\tLoss: 0.258712\n",
      "Train Epoch: 95 [10240/60000 (17%)]\tLoss: 0.260535\n",
      "Train Epoch: 95 [15360/60000 (25%)]\tLoss: 0.281696\n",
      "Train Epoch: 95 [20480/60000 (34%)]\tLoss: 0.292349\n",
      "Train Epoch: 95 [25600/60000 (42%)]\tLoss: 0.285020\n",
      "Train Epoch: 95 [30720/60000 (51%)]\tLoss: 0.254351\n",
      "Train Epoch: 95 [35840/60000 (59%)]\tLoss: 0.298287\n",
      "Train Epoch: 95 [40960/60000 (68%)]\tLoss: 0.236831\n",
      "Train Epoch: 95 [46080/60000 (76%)]\tLoss: 0.202864\n",
      "Train Epoch: 95 [51200/60000 (85%)]\tLoss: 0.308089\n",
      "Train Epoch: 95 [56320/60000 (93%)]\tLoss: 0.245140\n",
      "Test set: Avg. loss: 0.3407, Accuracy: 8782/10000 (87%)\n",
      "\n",
      "Train Epoch: 96 [0/60000 (0%)]\tLoss: 0.197358\n",
      "Train Epoch: 96 [5120/60000 (8%)]\tLoss: 0.263763\n",
      "Train Epoch: 96 [10240/60000 (17%)]\tLoss: 0.217933\n",
      "Train Epoch: 96 [15360/60000 (25%)]\tLoss: 0.298781\n",
      "Train Epoch: 96 [20480/60000 (34%)]\tLoss: 0.273742\n",
      "Train Epoch: 96 [25600/60000 (42%)]\tLoss: 0.293768\n",
      "Train Epoch: 96 [30720/60000 (51%)]\tLoss: 0.280299\n",
      "Train Epoch: 96 [35840/60000 (59%)]\tLoss: 0.259197\n",
      "Train Epoch: 96 [40960/60000 (68%)]\tLoss: 0.277387\n",
      "Train Epoch: 96 [46080/60000 (76%)]\tLoss: 0.307153\n",
      "Train Epoch: 96 [51200/60000 (85%)]\tLoss: 0.285949\n",
      "Train Epoch: 96 [56320/60000 (93%)]\tLoss: 0.268833\n",
      "Test set: Avg. loss: 0.3415, Accuracy: 8797/10000 (87%)\n",
      "\n",
      "Train Epoch: 97 [0/60000 (0%)]\tLoss: 0.262958\n",
      "Train Epoch: 97 [5120/60000 (8%)]\tLoss: 0.314460\n",
      "Train Epoch: 97 [10240/60000 (17%)]\tLoss: 0.234898\n",
      "Train Epoch: 97 [15360/60000 (25%)]\tLoss: 0.270302\n",
      "Train Epoch: 97 [20480/60000 (34%)]\tLoss: 0.272867\n",
      "Train Epoch: 97 [25600/60000 (42%)]\tLoss: 0.260072\n",
      "Train Epoch: 97 [30720/60000 (51%)]\tLoss: 0.287370\n",
      "Train Epoch: 97 [35840/60000 (59%)]\tLoss: 0.290652\n",
      "Train Epoch: 97 [40960/60000 (68%)]\tLoss: 0.296157\n",
      "Train Epoch: 97 [46080/60000 (76%)]\tLoss: 0.213474\n",
      "Train Epoch: 97 [51200/60000 (85%)]\tLoss: 0.223224\n",
      "Train Epoch: 97 [56320/60000 (93%)]\tLoss: 0.287588\n",
      "Test set: Avg. loss: 0.3409, Accuracy: 8775/10000 (87%)\n",
      "\n",
      "Train Epoch: 98 [0/60000 (0%)]\tLoss: 0.305574\n",
      "Train Epoch: 98 [5120/60000 (8%)]\tLoss: 0.307574\n",
      "Train Epoch: 98 [10240/60000 (17%)]\tLoss: 0.245476\n",
      "Train Epoch: 98 [15360/60000 (25%)]\tLoss: 0.296833\n",
      "Train Epoch: 98 [20480/60000 (34%)]\tLoss: 0.291815\n",
      "Train Epoch: 98 [25600/60000 (42%)]\tLoss: 0.261410\n",
      "Train Epoch: 98 [30720/60000 (51%)]\tLoss: 0.249125\n",
      "Train Epoch: 98 [35840/60000 (59%)]\tLoss: 0.305702\n",
      "Train Epoch: 98 [40960/60000 (68%)]\tLoss: 0.284079\n",
      "Train Epoch: 98 [46080/60000 (76%)]\tLoss: 0.254168\n",
      "Train Epoch: 98 [51200/60000 (85%)]\tLoss: 0.255477\n",
      "Train Epoch: 98 [56320/60000 (93%)]\tLoss: 0.294880\n",
      "Test set: Avg. loss: 0.3419, Accuracy: 8774/10000 (87%)\n",
      "\n",
      "Train Epoch: 99 [0/60000 (0%)]\tLoss: 0.300029\n",
      "Train Epoch: 99 [5120/60000 (8%)]\tLoss: 0.288440\n",
      "Train Epoch: 99 [10240/60000 (17%)]\tLoss: 0.265296\n",
      "Train Epoch: 99 [15360/60000 (25%)]\tLoss: 0.244627\n",
      "Train Epoch: 99 [20480/60000 (34%)]\tLoss: 0.254022\n",
      "Train Epoch: 99 [25600/60000 (42%)]\tLoss: 0.192688\n",
      "Train Epoch: 99 [30720/60000 (51%)]\tLoss: 0.317377\n",
      "Train Epoch: 99 [35840/60000 (59%)]\tLoss: 0.240157\n",
      "Train Epoch: 99 [40960/60000 (68%)]\tLoss: 0.251820\n",
      "Train Epoch: 99 [46080/60000 (76%)]\tLoss: 0.222074\n",
      "Train Epoch: 99 [51200/60000 (85%)]\tLoss: 0.304536\n",
      "Train Epoch: 99 [56320/60000 (93%)]\tLoss: 0.290849\n",
      "Test set: Avg. loss: 0.3374, Accuracy: 8809/10000 (88%)\n",
      "\n",
      "Train Epoch: 100 [0/60000 (0%)]\tLoss: 0.280805\n",
      "Train Epoch: 100 [5120/60000 (8%)]\tLoss: 0.314033\n",
      "Train Epoch: 100 [10240/60000 (17%)]\tLoss: 0.277778\n",
      "Train Epoch: 100 [15360/60000 (25%)]\tLoss: 0.278991\n",
      "Train Epoch: 100 [20480/60000 (34%)]\tLoss: 0.218625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 100 [25600/60000 (42%)]\tLoss: 0.280051\n",
      "Train Epoch: 100 [30720/60000 (51%)]\tLoss: 0.228522\n",
      "Train Epoch: 100 [35840/60000 (59%)]\tLoss: 0.269150\n",
      "Train Epoch: 100 [40960/60000 (68%)]\tLoss: 0.251009\n",
      "Train Epoch: 100 [46080/60000 (76%)]\tLoss: 0.296458\n",
      "Train Epoch: 100 [51200/60000 (85%)]\tLoss: 0.256461\n",
      "Train Epoch: 100 [56320/60000 (93%)]\tLoss: 0.280690\n",
      "Test set: Avg. loss: 0.3447, Accuracy: 8749/10000 (87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4FxFNbjHGVZ",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f630ff326d8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "final_train_counter = []\n",
    "final_train_losses = []\n",
    "final_test_counter = []\n",
    "final_test_losses = []\n",
    "\n",
    "for i in range(0, n_epochs):\n",
    "  final_train_counter.append(i)\n",
    "\"\"\"\n",
    "for i in range(0, len(avg_train_loss)-1, 30):\n",
    "    index = int(i/10)\n",
    "    final_train_counter.append(train_counter[i])\n",
    "    final_train_losses.append((train_losses[i] + train_losses[i+1]) / 2)\n",
    "    final_test_counter.append(train_counter[i])\n",
    "    final_test_losses.append((test_losses[i] + test_losses[i+1]) / 2)\n",
    "\n",
    "final_train_counter.append(train_counter[len(train_counter)-1]) \n",
    "final_train_losses.append(train_losses[len(train_counter)-1])\n",
    "final_test_counter.append(train_counter[len(train_counter)-1])\n",
    "final_test_losses.append(test_losses[len(train_counter)-1])    \n",
    "    \n",
    "plt.plot(len(avg_train_loss), avg_train_loss, color='blue')\n",
    "plt.scatter(avg_train_loss[-1], avg_train_loss[-1], color='blue')\n",
    "plt.plot(len(avg_test_losses), avg_test_losses, color='red')\n",
    "plt.scatter(avg_test_losses[-1], avg_test_losses[-1], color='red')\n",
    "\n",
    "# print(len(test_counter))\n",
    "# print(len(test_losses))\n",
    "\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')\n",
    "\"\"\"\n",
    "\n",
    "plt.plot(final_train_counter, avg_train_loss, color='blue')\n",
    "plt.scatter(final_train_counter[-1], avg_train_loss[-1], color='blue')\n",
    "plt.plot(final_train_counter, avg_test_loss, color='red')\n",
    "plt.scatter(final_train_counter[-1], avg_test_loss[-1], color='red')\n",
    "# plt.plot(final_train_counter, test_accuracy_list, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fU20QrRFnQEL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f630fe96dd8>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGZBJREFUeJzt3XuUFOWZx/HvI8OAoAQQglx0wSzBC5pBJkajZzfooojXTUwWlYQxibjZxKhZ3ZDLUZPjnrPxGKNudslh1Rl0DXHFa0yMSYhZ3U0WHZwRESGADDcRxxugKNdn/3i7M8NM93QPM9XVVf37nNOnp2qqp56y8Dc1b73vW+buiIhI8h0UdwEiItI7FOgiIimhQBcRSQkFuohISijQRURSQoEuIpISCnQRkZRQoIuIpIQCXUQkJapKubNhw4b52LFjS7lLEZHEW7JkyRvuPrzQdiUN9LFjx9LY2FjKXYqIJJ6ZrStmOzW5iIikhAJdRCQlFOgiIimhQBcRSQkFuohISijQRURSQoEuIpISCnQRkSi99x5ccw2sXh35rhToIiJRWrgQbrsNNm+OfFcKdBGRKDU0wEc+AqedFvmuFOgiIlF55RX4/e+hrg7MIt+dAl1EJCr33BOC/AtfKMnuFOgiIlHYty80t5xxBhx5ZEl2qUAXEYnCf/83rFsHl11Wsl0q0EVEolBfD4MGwYUXlmyXJZ0PXURi8MEHsHEj/OVfFv+ZF16A11+PrqaO+vYNvUCqOkTSunUwdCgceuj+61tbobm5dPV11549obvi5z8PAwaUbLcKdJG0++534cc/hvXr4cMfLrz9mjVw4omhDbiU7rgDrryybfn990Md06fDvffuv+1FF8HTT5e2vgPxxS+WdHcKdJE027UL5s+HnTvhvvvCiMVCGhrC+xNPdL4yjsqVV8Jdd+0f6A8/DG+9Fa50//VfYfDgsH7VqhDmV18dgr1cfehDMHFiSXepQBdJs1/+Et54I7Tl1teHEOyqP/TeveEXwJlnwrRppavzy1+Gr34Vmppg0qSwrqEh1L1tG9x/P1xxRVg/fz4cdBBcey2MHl26GhNAN0VF0qy+Hg4/HG66CV58MQRmV556CjZsCANhSmnGDKiubvvrYP16+O1vwy+giRPDccD+v3AU5p0o0EXSassW+MUvwo25mTOhX7+2wMynvj40bVxwQUlK/LOhQ0NvkPvuC81E994L7uEXS10dLF4ML78Mv/tduMFbwq6ASaJAF0mr++4LV7SXXQZDhrQF5s6dubd/5x146CG4+GLo37+0tUII7jffhMceC794PvUpGDcu/DLq0yesy/7COf/80teXAAp0kTRyD+H3iU/AMceEdXV14Sbj44/n/sx//Vfo4hjX1e+ZZ8KoUXDddWGq2Wyzz4gRoafL/PnhRukll8TzCycBdFNU0mffvv2vQvv3z30j8IMPQvCl0QsvwLJlMHdu27qpU0Ng3n13CMiO6uvh2GOhtrZ0dbbXp09oHvrBD2DgQPjMZ9q+V1cHP/9529eSm7uX7DV58mQXKejJJ92HDXNfty7/Ntu2uR9xhPvdd3f+3pQp7iGqw+vccztvc/vt+2+Txle/fu5vv73/cc+Z0/Vnbr65Z+eup1asCHXU1e2/fufO8G/iuOPc9+2Lp7YYAY1eRMbqCl3Kzx13hK529fVwww25t3nggdAb4/bb928iWLYs9NSYMQNqauC55+DBB8M0pkcdFbZxD/2ajz8eLr00+uOJywkntPXdzrruOhg+HHbv7rx9v35w+eWlqS2fCRPgkUdCU1F71dWhbX3gwJJMQ5tYxaR+b710hS4Fvfqqe58+7mbu48a5792be7vTTgvbgHtTU9v6b3zDvW9f99bWsLxhg/tBB7lff33bNs88Ez7X0BDdcYj0Ioq8QtdNUSkv//mfoWfGd78La9fmHt69ejX8z/+Eq83q6rY+yrt3h8+fdx4MGxbWjRkT2o7nz28byt7QAIccUt6jDEUOgAJdyod7CNtTToE5c8IowVz9phsawkjBq64K/aWzfZefeCJMKNXxplldXZjk6fe/Dw/svf9++Oxnw5/vIilSVKCb2TVm9pKZLTOzBWbW38wazGytmTVnXjVRFysp99xzsHx5aBMfMAD+7u9CW/n27W3bZEcKnnVW6LFx2WWh7/Ljj4egHzGi85D1Cy8M82rU14d+1u++q4EpkkoFA93MRgNfB2rdfSLQB5iR+fZ17l6TeZXxXJaSCA0NoYvh5z4XluvqYMeOMDlTVnakYPYqfOpUGDkSbr01dGubOTNMxdpe//5hsMyDD4ZZB486qiQP7BUptWKbXKqAg82sChgAvBpdSVKRPvgAFiyAT386XE1DaHr56Efb2sih80jBqqrwvMb//d8wB3W+Psp1dWE61mefLdkDe0VKrWC3RXffZGa3AOuB94Ffu/uvzewS4J/N7HpgETDH3fOMKZZEW70aZs3KP2S8N+zYEYaet28KMQvh++1vw+TJYXnp0tC1rv1Iwbq6MBiltjb/dKUnnRRGTK5YEY5FJIUKBrqZDQEuAMYB7wAPmNlM4FvAa0A1MA/4JvD9HJ+fDcwGOLJED0qVXvbjH4f27TPPjHY/f/3XMGXK/usuvzzMELhjR1g+8sjOc3offTR8//vwyU/m/9lm8MMfhl8I+ncoKWWhi2MXG5h9Fpjm7l/KLH8BONnd/6HdNp8CrnX3c7v6WbW1td7Y2NjjoqWEdu0K05ROmRLm+hCRkjOzJe5ecE6GYtrQ1wMnm9kAMzPgDOBlMxuZ2ZEBFwLLelKwlKnHHw+jNtUrRKTsFdOGvtjMFgLPA3uAJkITyxNmNhwwoBn4+ygLlZg0NIReJFOnxl2JiBRQ1Fwu7n4D0HFSjdN7vxwpK6+9Fh5hdu21nZ/GLiJlRyNFJb/sMHxNVyqSCAp0yS07DP/kk0MvEhEpe/o7upytWQP//u/hKrkr1dVt06K2d+edYTrZA/Hee/DSS/CTnxzY50Wk5BTo5ezGG+GnP4VDD+16u61bQ6jfdFPbunXrYPbsMACnuvrA9j9hQphPRUQSQYFerrZuDXOPXHFFuErvyjnnhAmrvve98BgvgHvuCc0my5fD2LGRlysi8VMberl64IEw90gxNyTr6sKEVYsWheVs+/eUKQpzkQqiQC9X9fVh7pGPf7zwtuedB0OGtM0d/swz4ZFrGgwkUlEU6OXoT3+CP/whBHIxswL27w+XXAIPPxwmuGpoCO3un/505KWKSPlQoJejhobQFj5zZvGfqasLU9DedVeYc+Vzn9MTeUQqjG6Klpu9e8MNzWnTwpD7Yk2eHKaO/c53wjS3am4RqTgK9KgtWQItLcVvv2oVbNoEt93Wvf1k5w6/9loYP77rqWRFJJUU6FHatQtOPbX7D4YYMSLc6OyumTPh+utD/3M9kUek4ijQo7RhQwjzm25qe2RaMQ4/HPr16/7+RowIA4qGDu3+Z0Uk8RToUco2tZx6Khx/fGn2OWxYafYjImVHvVyitHZteNfgHhEpAQV6lFpaQvfDMWPirkREKoACPUotLXDEEXo4hIiUhAI9Si0tam4RkZLRpWN3vfkm7NnTeX2/fjB48P7rWlr0LE4RKRldoXdHfX3oRXL44Z1fQ4fCH//Ytu3OnfDqq7pCF5GS0RV6d8ydCx/9KFx99f7rd++Gq66Cp5+GU04J69avD9PYKtBFpEQU6MV66SV47jm49Vb4ylc6f//WW6G5uW052wddgS4iJaIml2I1NITeKvlmQKypyR3o48ZFXZmICKBAL87u3XDvvXDuuZ0fxJxVUwMrV4aHK0MI9KoqGDWqZGWKSGVToBfjySdhy5auHwc3aVJoM3/xxbC8dq36oItISSnQi1FfH67Mp0/Pv01NTXjPNruoD7qIlJgCvZA33oCf/zy0nfftm3+7I48M/dAV6CISE7UHZN14I/z0p53Xv/deaEMv9AQgs7Ybox98AJs364aoiJSUAh1g61a4+ebQx/zYYzt/f8KE4qa/nTQJfvITzbIoIrFQoEN4qPL778O8eXDSSQf+c2pqws958smwrEAXkRJSGzqEPubHHgsf/3jPfk72xugjj4R3BbqIlJACfeVK+MMfQpfEnj6H8+ijoboannlGfdBFpOSKCnQzu8bMXjKzZWa2wMz6m9k4M1tsZqvM7H4zq4662EjMnx8eQpFvBGh3VFfDccfBvn2h10ufPj3/mSIiRSoY6GY2Gvg6UOvuE4E+wAzgB8CP3H088DbwpSgLjcTevXDPPTBtGowc2Ts/c9Kk8K4eLiJSYsU2uVQBB5tZFTAA2AycDizMfH8+cGHvlxex3/4WNm0q3CWxO7Lt6Go/F5ESKxjo7r4JuAVYTwjyrcAS4B13zz7pYSMwOqoiI1NfD4cdBued13s/U4EuIjEppsllCHABMA4YBQwEzs6xqef5/GwzazSzxtbW1p7U2rvefjv0RrnkktD23VsmT4bTT4czz+y9nykiUoRi+qH/DbDW3VsBzOwh4JPAYDOrylyljwFezfVhd58HzAOora3NGfqx+NnPwlOFerO5BWDAAFi0qHd/pohIEYppQ18PnGxmA8zMgDOA5cBTwEWZbWYBj0ZTYkQaGuCEE9qaSEREEq6YNvTFhJufzwMvZj4zD/gm8A0zWw0cBtwVYZ29a/lyePbZcHXe077nIiJloqih/+5+A3BDh9WvAD0YJx+j+vow8OfSS+OuRESk11TeSNE9ewo/fUhEJIEqL9B/9avCTx8SEUmgypht8de/DgOIINwMLfT0IRGRBEp/oK9YAWedtf+6b36z66cPiYgkUPoDvaEhTJK1ZEl4RJwZjBkTd1UiIr0u3YG+Z0+YfGv6dPjYx+KuRkQkUum+Kfqb34Rne/b2aFARkTKU7kCvr4dhw+Ccc+KuREQkcukN9LfegkcfDYOHenPyLRGRMpXeQF+wAHbtUn9zEakY6Q30hoYw8ZYm3xKRCpHOQF+zBhobYdasuCsRESmZdAZ6Y2N4nzIl3jpEREoonYHe1BRGgh5zTNyViIiUTDoDvbkZjjtOvVtEpKKkN9B1M1REKkz6Av2118L0uJMmxV2JiEhJpS/Qm5rCu67QRaTCpC/Qm5vDuybjEpEKk85AHzcOPvShuCsRESmpdAa62s9FpAKlK9DffRdWrVL7uYhUpHQF+tKl4K5AF5GKlK5Az94QVZOLiFSgdAV6UxMcdhiMHh13JSIiJZeuQM+OEDWLuxIRkZJLT6Dv2QMvvqj2cxGpWOkJ9JUrYedOtZ+LSMVKT6Bnh/xrhKiIVKj0BHpzM/TrB0cfHXclIiKxSFegH388VFXFXYmISCzSEejuoclF7eciUsHSEegbN8Jbb6mHi4hUtILtE2Y2Abi/3aqjgOuBwcDlQGtm/bfd/Ze9XmExsiNEFegiUsEKBrq7rwRqAMysD7AJeBi4DPiRu98SaYXFaGoKg4lOOCHuSkREYtPdJpczgDXuvi6KYg5YczOMHw+HHBJ3JSIiseluoM8AFrRb/pqZLTWzu81sSK4PmNlsM2s0s8bW1tZcm/ScHgotIlJ8oJtZNXA+8EBm1VzgI4TmmM3AD3N9zt3nuXutu9cOHz68h+Xm8M47sHatAl1EKl53rtDPBp539y0A7r7F3fe6+z7gP4CToiiwoBdeCO/qsigiFa47gX4x7ZpbzGxku+/9LbCst4rqFvVwEREBiujlAmBmA4CpwBXtVt9sZjWAAy0dvlc6zc0wYgQcfngsuxcRKRdFBbq77wAO67Du85FU1F1NTbo6FxEh6SNFd+2C5cvVfi4iQtIDffly2L1bV+giIiQ90JcuDe+aA11EJOGB/sorYcj/uHFxVyIiErtkB3pLC4waFR5sISJS4ZIf6Lo6FxEB0hDoY8fGXYWISFlIbqDv2RMebKFAFxEBkhzoGzfC3r0KdBGRjOQG+tq14V2BLiICJDnQW1rCu26KiogASQ/0gw6CMWPirkREpCwkO9BHj4bq6rgrEREpC8kOdLWfi4j8mQJdRCQlkhnou3eHbou6ISoi8mfJDPQNG2DfPl2hi4i0k8xAz3ZZVKCLiPyZAl1EJCWSG+jqgy4isp/kBvoRR0DfvnFXIiJSNpIb6GpuERHZTzIDfe1aBbqISAfJC/Rdu2DTJgW6iEgHyQv0DRvAXYEuItJB8gJd0+aKiOSU3EDXFbqIyH6SF+ibNoX3UaPirUNEpMwkL9C3bYOBA9UHXUSkg+QF+vbtcOihcVchIlJ2khfo27bBoEFxVyEiUnYU6CIiKVEw0M1sgpk1t3ttM7OrzWyomf3GzFZl3oeUomAFuohIbgUD3d1XunuNu9cAk4EdwMPAHGCRu48HFmWWo7dtm9rQRURy6G6TyxnAGndfB1wAzM+snw9c2JuF5bV9u67QRURy6G6gzwAWZL4e4e6bATLvH871ATObbWaNZtbY2tp64JVmqclFRCSnogPdzKqB84EHurMDd5/n7rXuXjt8+PDu1tfxhynQRUTy6M4V+tnA8+6+JbO8xcxGAmTeX+/t4jrZuRN271agi4jk0J1Av5i25haAx4BZma9nAY/2VlF5bdsW3nVTVESkk6IC3cwGAFOBh9qt/hdgqpmtynzvX3q/vA62bw/vukIXEemkqpiN3H0HcFiHdW8Ser2UTvYKXYEuItJJskaKKtBFRPJSoIuIpEQyA103RUVEOklWoOumqIhIXskKdDW5iIjklbxA79MHDj447kpERMpO8gJ90CAwi7sSEZGyk6xA1+PnRETySlaga2IuEZG8FOgiIimhQBcRSYnkBbra0EVEckpWoOvxcyIieSUr0NXkIiKSV3ICfd8+XaGLiHQhOYH+7rvhXYEuIpJTcgJdMy2KiHQpOYGumRZFRLqUnEDXTIsiIl1SoIuIpIQCXUQkJZIX6LopKiKSU3ICXTdFRUS6lJxA1xW6iEiXkhXoBx8MffvGXYmISFlKVqCruUVEJK9kBbqaW0RE8kpOoGtiLhGRLiUn0NXkIiLSJQW6iEhKJCvQ1YYuIpJXsgJdV+giInkVFehmNtjMFprZCjN72cxOMbMbzWyTmTVnXtMjrVQ3RUVEulRV5Ha3A79y94vMrBoYAJwF/Mjdb4msuqydO8NLgS4iklfBQDezQcBfAXUA7r4L2GVm0VbWnuZxEREpqJgml6OAVqDezJrM7E4zG5j53tfMbKmZ3W1mQyKrUvO4iIgUVEygVwEnAnPdfRLwHjAHmAt8BKgBNgM/zPVhM5ttZo1m1tja2npgVWoudBGRgooJ9I3ARndfnFleCJzo7lvcfa+77wP+Azgp14fdfZ6717p77fDhww+sSjW5iIgUVDDQ3f01YIOZTcisOgNYbmYj2232t8CyCOoLdIUuIlJQsb1crgTuy/RweQW4DLjDzGoAB1qAKyKpEBToIiJFKCrQ3b0ZqO2w+vO9X04euikqIlJQMkaKqg1dRKSgZAT6tm1gBgMHFt5WRKRCJSfQBw0KoS4iIjklI9AnToSLLoq7ChGRspaMQP/yl+HOO+OuQkSkrCUj0EVEpCAFuohISijQRURSQoEuIpISCnQRkZRQoIuIpIQCXUQkJRToIiIpYe5eup2ZtQLrDvDjw4A3erGcpNBxV55KPXYdd35/4e4FnxBU0kDvCTNrdPeOU/imno678lTqseu4e05NLiIiKaFAFxFJiSQF+ry4C4iJjrvyVOqx67h7KDFt6CIi0rUkXaGLiEgXEhHoZjbNzFaa2WozmxN3PVExsyPM7Ckze9nMXjKzqzLrh5rZb8xsVeZ9SNy1RsHM+phZk5k9nlkeZ2aLM8d9v5lVx11jbzOzwWa20MxWZM77KZVwvs3smsy/8WVmtsDM+qfxfJvZ3Wb2upkta7cu5/m14I5Mzi01sxO7u7+yD3Qz6wP8G3A2cCxwsZkdG29VkdkD/KO7HwOcDHw1c6xzgEXuPh5YlFlOo6uAl9st/wD4Uea43wa+FEtV0bod+JW7Hw18jHD8qT7fZjYa+DpQ6+4TgT7ADNJ5vhuAaR3W5Tu/ZwPjM6/ZwNzu7qzsAx04CVjt7q+4+y7gZ8AFMdcUCXff7O7PZ77eTvifezTheOdnNpsPXBhPhdExszHAOcCdmWUDTgcWZjZJ3XGb2SDgr4C7ANx9l7u/QwWcb6AKONjMqoABwGZSeL7d/WngrQ6r853fC4B7PPg/YLCZjezO/pIQ6KOBDe2WN2bWpZqZjQUmAYuBEe6+GULoAx+Or7LI3Ab8E7Avs3wY8I6778ksp/G8HwW0AvWZpqY7zWwgKT/f7r4JuAVYTwjyrcAS0n++s/Kd3x5nXRIC3XKsS3XXHDM7BHgQuNrdt8VdT9TM7FzgdXdf0n51jk3Tdt6rgBOBue4+CXiPlDWv5JJpM74AGAeMAgYSmhs6Stv5LqTH/+aTEOgbgSPaLY8BXo2plsiZWV9CmN/n7g9lVm/J/umVeX89rvoicipwvpm1EJrUTidcsQ/O/EkO6TzvG4GN7r44s7yQEPBpP99/A6x191Z33w08BHyS9J/vrHznt8dZl4RAfw4Yn7kDXk24efJYzDVFItNufBfwsrvf2u5bjwGzMl/PAh4tdW1RcvdvufsYdx9LOL+/c/dLgaeAizKbpfG4XwM2mNmEzKozgOWk/HwTmlpONrMBmX/z2eNO9fluJ9/5fQz4Qqa3y8nA1mzTTNHcvexfwHTgT8Aa4Dtx1xPhcZ5G+BNrKdCceU0ntCcvAlZl3ofGXWuE/w0+BTye+foo4FlgNfAA0C/u+iI43hqgMXPOHwGGVML5Br4HrACWAfcC/dJ4voEFhPsEuwlX4F/Kd34JTS7/lsm5Fwm9gLq1P40UFRFJiSQ0uYiISBEU6CIiKaFAFxFJCQW6iEhKKNBFRFJCgS4ikhIKdBGRlFCgi4ikxP8DYQ89eYLSbOIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "last_acc = []\n",
    "last_counter = []\n",
    "index = 0\n",
    "for acc in range(0, len(test_accuracy_list)-1) :\n",
    "  last_acc.append(test_accuracy_list[acc])\n",
    "  last_counter.append(final_train_counter[index])\n",
    "  index = index + 1\n",
    "\n",
    "plt.plot(last_counter, last_acc, color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SavedAccuracy():\n",
    "    \n",
    "    def __init__(self, last_acc, last_counter, name=\"data.pkl\"):\n",
    "        self.last_acc = last_acc\n",
    "        self.last_counter = last_counter\n",
    "        self.name = name\n",
    "    \n",
    "    def saveGraph(self):\n",
    "        graph = {'last_acc': self.last_acc, 'last_counter': self.last_counter}\n",
    "        torch.save(graph, \"./acc_data/\"+ self.name) \n",
    "        \n",
    "    def loadGraph(self):\n",
    "        graph = torch.load(\"./acc_data/\"+ self.name)\n",
    "        last_acc_fetch = graph['last_acc']\n",
    "        last_counter_fetch = graph['last_counter']\n",
    "\n",
    "        return (last_acc_fetch, last_counter_fetch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = SavedAccuracy(last_acc, last_counter, name='avg_dropout_acc.pkl')\n",
    "acc1.saveGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJa35oEyHGVf"
   },
   "outputs": [],
   "source": [
    "def makeCifar10Dec(param):\n",
    "    if param == 0:\n",
    "        return \"airplane\"\n",
    "    elif param == 1:\n",
    "        return \"automobile\"\n",
    "    elif param == 2:\n",
    "        return \"bird\"\n",
    "    elif param == 3:\n",
    "        return \"cat\"\n",
    "    elif param == 4:\n",
    "        return \"deer\"\n",
    "    elif param == 5:\n",
    "        return \"dog\"\n",
    "    elif param == 6:\n",
    "        return \"frog\"\n",
    "    elif param == 7:\n",
    "        return \"horse\"\n",
    "    elif param == 8:\n",
    "        return \"ship\"\n",
    "    elif param == 9:\n",
    "        return \"truck\"\n",
    "    \n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "for i in range(0, 10):\n",
    "    batch_idx, (example_data, example_targets) = next(examples)\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        output = network(example_data)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        for i in range(6):\n",
    "            plt.subplot(2,3,i+1)\n",
    "            plt.tight_layout()\n",
    "            plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "            plt.title(\"Prediction: {}\".format(\n",
    "                output.data.max(1, keepdim=True)[1][i].item()))\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Adlhca7pHGVo"
   },
   "outputs": [],
   "source": [
    "# Reloading a already saved model\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "continued_network = Net()\n",
    "continued_optimizer = optim.SGD(continued_network.parameters(), lr=learning_rate,\n",
    "                                momentum=momentum)\n",
    "\n",
    "network_state_dict = torch.load('./results/model.pth')\n",
    "continued_network.load_state_dict(network_state_dict)\n",
    "\n",
    "optimizer_state_dict = torch.load('./results/optimizer.pth')\n",
    "continued_optimizer.load_state_dict(optimizer_state_dict)\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "for i in range(0, 2):\n",
    "    batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = continued_network(example_data)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        for i in range(6):\n",
    "            plt.subplot(2,3,i+1)\n",
    "            plt.tight_layout()\n",
    "            plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "            plt.title(\"Prediction: {}\".format(\n",
    "                output.data.max(1, keepdim=True)[1][i].item()))\n",
    "            \n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLlTgHruHGVt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Averaged TABU NN Dropout.ipynb",
   "provenance": [
    {
     "file_id": "165_z7nSqC3c0e88BEkLaVk6VLbOzPnF-",
     "timestamp": 1554971127153
    },
    {
     "file_id": "1GUow23zovvlLuNkMUkHz4zJzfehTEQl9",
     "timestamp": 1554791975444
    },
    {
     "file_id": "1zhoOPcwneUN3foYHbrjQWttdHus2hocz",
     "timestamp": 1554750187224
    },
    {
     "file_id": "1vRXEE00eunJuYh-9toP6O6KlXIJHwTjG",
     "timestamp": 1554747644571
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
