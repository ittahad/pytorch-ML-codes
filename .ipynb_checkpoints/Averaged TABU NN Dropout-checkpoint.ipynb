{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3751,
     "status": "ok",
     "timestamp": 1555138488774,
     "user": {
      "displayName": "Ittahaduzzaman Akash",
      "photoUrl": "https://lh3.googleusercontent.com/-YERuM7ONE4k/AAAAAAAAAAI/AAAAAAAAAM8/wlRqU6U6u18/s64/photo.jpg",
      "userId": "11972750857132575782"
     },
     "user_tz": -360
    },
    "id": "FeFfkAp7HGSl",
    "outputId": "1c6e83db-af88-44a5-ac9a-cf7430d538f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0068206c90>"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "tabu1 = []\n",
    "tabu2 = []\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size_train = 512\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kN22sGD4O1Re"
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.FashionMNIST('./data', train=True, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.FashionMNIST('./data', train=False, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# print(train_loader)\n",
    "# examples = enumerate(train_loader)\n",
    "# batch_idx, (example_data, example_targets) = next(examples)\n",
    "# print(example_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z0Q1QfHSHGS6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.CIFAR100('./data', train=True, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.CIFAR100('./data', train=False, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# print(train_loader)\n",
    "# examples = enumerate(train_loader)\n",
    "# batch_idx, (example_data, example_targets) = next(examples)\n",
    "# print(example_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5591,
     "status": "ok",
     "timestamp": 1555138490633,
     "user": {
      "displayName": "Ittahaduzzaman Akash",
      "photoUrl": "https://lh3.googleusercontent.com/-YERuM7ONE4k/AAAAAAAAAAI/AAAAAAAAAM8/wlRqU6U6u18/s64/photo.jpg",
      "userId": "11972750857132575782"
     },
     "user_tz": -360
    },
    "id": "dm1RiT4cHGTK",
    "outputId": "43b41ab5-3dd9-43cf-e47f-5868110d3774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f00223755f8>\n",
      "torch.Size([512, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.CIFAR10('./data', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.CIFAR10('./data', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "print(train_loader)\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PwEvmG00HGTY"
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.SVHN('./data', split='train', download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.SVHN('./data', split='test', download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# print(train_loader)\n",
    "# examples = enumerate(train_loader)\n",
    "# batch_idx, (example_data, example_targets) = next(examples)\n",
    "# print(example_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eV5zVHMxHGTj"
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#   torchvision.datasets.MNIST('./data', train=False, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# print(train_loader)\n",
    "# examples = enumerate(train_loader)\n",
    "# batch_idx, (example_data, example_targets) = next(examples)\n",
    "# print(example_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dl7nDBdAHGTw"
   },
   "outputs": [],
   "source": [
    "def initMatrix(size, append_with):\n",
    "    ary = []\n",
    "    for i in range(0, size):\n",
    "        ary.append(append_with)\n",
    "    return ary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LrjISTO9HGT6"
   },
   "outputs": [],
   "source": [
    "from torch.nn.modules import Module\n",
    "from torch.nn import functional as F\n",
    "from torch._jit_internal import weak_module, weak_script_method\n",
    "\n",
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5, inplace=False):\n",
    "        super(Dropout, self).__init__()\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"dropout probability has to be between 0 and 1, \"\n",
    "                             \"but got {}\".format(p))\n",
    "        self.p = p\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, input):\n",
    "        varTemp = F.dropout(input, self.p, self.training, self.inplace)\n",
    "        return varTemp\n",
    "\n",
    "    def __repr__(self):\n",
    "        inplace_str = ', inplace' if self.inplace else ''\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'p=' + str(self.p) \\\n",
    "            + inplace_str + ')'\n",
    "    \n",
    "class MyLinear(torch.nn.Linear):\n",
    "    def __init__(self, in_feats, out_feats, drop_p, t, bias=True):\n",
    "        super(MyLinear, self).__init__(in_feats, out_feats, bias=bias)\n",
    "        self.masker = Dropout(p=drop_p)\n",
    "        self.tabu = t\n",
    "        self.firstItr = True\n",
    "        self.network_dropout = drop_p\n",
    "        self.historial_tabu_count = initMatrix(1024, 0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print(self.tabu)\n",
    "        #print(input)\n",
    "        \n",
    "       \n",
    "        masked_weight = self.masker(self.weight)\n",
    "        #print(masked_weight)\n",
    "        output = F.linear(input, masked_weight, self.bias)\n",
    "        \n",
    "        # Here goes TABU\n",
    "        if self.firstItr == True:\n",
    "            \n",
    "            self.firstItr = False\n",
    "            for i in range(0, len(output[0])):\n",
    "                if output[0][i] < 0:\n",
    "                    # Neuron will be dropped\n",
    "                    self.tabu[i] = 0\n",
    "        else:\n",
    "            \n",
    "            temp_tabu = initMatrix(len(output[0]), 1)\n",
    "            \n",
    "            for i in range(0, len(output[0])):\n",
    "                if output[0][i] < 0:\n",
    "                    # Neuron will be dropped\n",
    "                    temp_tabu[i] = 0\n",
    "\n",
    "            for i in range(0, len(output[0])):\n",
    "                if (self.tabu[i] == 0 and temp_tabu[i] == 0):\n",
    "                    # Neuron will be dropped\n",
    "                    self.historial_tabu_count[i] = self.historial_tabu_count[i] + 1\n",
    "                    \n",
    "                    if self.historial_tabu_count[i] > 3:\n",
    "                      \n",
    "                      self.tabu[i] = 1\n",
    "                      output[0][i] = input[0][i]\n",
    "                      self.historial_tabu_count[i] = 0\n",
    "                else:\n",
    "                    \n",
    "                    \n",
    "            #print(temp_tabu)\n",
    "            \n",
    "        #print(self.historial_tabu_count)    \n",
    "     \n",
    "            \n",
    "        #print(output)\n",
    "        #print(self.tab5u)\n",
    "        \n",
    "        self.firstItr = False\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPm0tWSOHGUF"
   },
   "outputs": [],
   "source": [
    "tabu1 = initMatrix(1024, 1)\n",
    "tabu2 = initMatrix(1024, 1)\n",
    "\n",
    "layer1_dropout = 0.5\n",
    "layer2_dropout = 0.5\n",
    "\n",
    "#tabu3 = initMatrix(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ei-XUUvmHGUO"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "       \n",
    "        # define the layers and their sizes, turn off bias\n",
    "        \n",
    "        self.fc1 = nn.Linear(3072, 1024)\n",
    "       \n",
    "        self.aD1 = MyLinear(1024, 1024, layer1_dropout, tabu1) #nn.Dropout(0.5) # # #      \n",
    "        \n",
    "        #self.d2 = nn.Dropout(0.5)\n",
    "        self.aD2 = MyLinear(1024, 1024, layer2_dropout, tabu2) \n",
    "        #self.fc2 = nn.Linear(1024, 1024)\n",
    "        #self.d2 = Dropout(0.5) \n",
    "        #self.fc3 = nn.Linear(1024, 1024)\n",
    "        \n",
    "        #For CIFAR-10\n",
    "        #self.d3 = Dropout(0.3) \n",
    "        #self.fc3_1 = nn.Linear(512, 256)\n",
    "        \n",
    "        self.fc4 = nn.Linear(1024, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3072)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(\"First TABU Layer\")\n",
    "        x = F.relu(self.aD1(x))\n",
    "        #print(\"Second TABU Layer\")\n",
    "        #x = F.relu(self.dT(x))\n",
    "        x = F.relu(self.aD2(x))\n",
    "        \n",
    "        #x = F.relu(self.d2(x))\n",
    "        #x = F.relu(self.fc3(x))\n",
    "        \n",
    "        # for CIFAR-10\n",
    "        #x = F.relu(self.d3(x))\n",
    "        #x = F.relu(self.fc3_1(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9jp8qjX7HGUa"
   },
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HgE1I92yHGVA"
   },
   "outputs": [],
   "source": [
    "avg_train_loss = []\n",
    "avg_train_counter = []\n",
    "avg_test_loss = []\n",
    "avg_test_counter = []\n",
    "test_accuracy_list = []\n",
    "\n",
    "test_losses = []\n",
    "test_counter = []\n",
    "\n",
    "epoch_number = 0\n",
    "train_avg_loss = 0\n",
    "test_avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aGOcLvQ2HGVG"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  \n",
    "    train_losses = []\n",
    "    train_counter = []\n",
    "    \n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            #torch.save(network.state_dict(), './results/cifar100_with_dropout_model.pth')\n",
    "            #torch.save(optimizer.state_dict(), './results/cifar100_with_dropout_optimizer.pth')\n",
    "            test(epoch, False)\n",
    "            \n",
    "    sum = 0.0\n",
    "    for loss in train_losses:\n",
    "      sum = sum + loss\n",
    "    \n",
    "    avg_train_loss.append(sum / len(train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwqm3AuCHGVM"
   },
   "outputs": [],
   "source": [
    "def test(epoch, graphOp):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        #for data, target in test_loader:\n",
    "        iterator = iter(test_loader)\n",
    "        data, target= iterator.next() \n",
    "        \n",
    "        examples = enumerate(test_loader)\n",
    "        batch_idx, (example_data, example_targets) = next(examples)  \n",
    "\n",
    "        output = network(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "        test_loss /= batch_size_test\n",
    "        test_losses.append(test_loss)\n",
    "        test_counter.append((batch_idx*64) + ((epoch-1)*len(test_loader.dataset)))\n",
    "        \n",
    "        if graphOp == True:\n",
    "          avg_test_loss.append(test_loss)\n",
    "          test_accuracy_list.append(100. * correct / batch_size_test)\n",
    "    \n",
    "    print('Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, batch_size_test,\n",
    "        100. * correct / batch_size_test))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2536
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1093203,
     "status": "ok",
     "timestamp": 1555083207914,
     "user": {
      "displayName": "Ittahaduzzaman Akash",
      "photoUrl": "https://lh3.googleusercontent.com/-YERuM7ONE4k/AAAAAAAAAAI/AAAAAAAAAM8/wlRqU6U6u18/s64/photo.jpg",
      "userId": "11972750857132575782"
     },
     "user_tz": -360
    },
    "id": "77ZTDVwnHGVS",
    "outputId": "0ceebe6d-b3a4-46a0-8c2b-7f4303483729",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.315969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Avg. loss: 2.3002, Accuracy: 102/1000 (10%)\n",
      "\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 2.288727\n",
      "Test set: Avg. loss: 2.2897, Accuracy: 126/1000 (12%)\n",
      "\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 2.286428\n",
      "Test set: Avg. loss: 2.2872, Accuracy: 125/1000 (12%)\n",
      "\n",
      "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 2.271415\n",
      "Test set: Avg. loss: 2.2727, Accuracy: 145/1000 (14%)\n",
      "\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 2.265148\n",
      "Test set: Avg. loss: 2.2649, Accuracy: 152/1000 (15%)\n",
      "\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 2.265057\n",
      "Test set: Avg. loss: 2.2578, Accuracy: 148/1000 (14%)\n",
      "\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 2.258959\n",
      "Test set: Avg. loss: 2.2591, Accuracy: 134/1000 (13%)\n",
      "\n",
      "Train Epoch: 1 [35840/50000 (71%)]\tLoss: 2.248888\n",
      "Test set: Avg. loss: 2.2463, Accuracy: 166/1000 (16%)\n",
      "\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 2.235698\n",
      "Test set: Avg. loss: 2.2319, Accuracy: 202/1000 (20%)\n",
      "\n",
      "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 2.232803\n",
      "Test set: Avg. loss: 2.2222, Accuracy: 200/1000 (20%)\n",
      "\n",
      "Test set: Avg. loss: 2.2125, Accuracy: 228/1000 (22%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.243507\n",
      "Test set: Avg. loss: 2.2138, Accuracy: 203/1000 (20%)\n",
      "\n",
      "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 2.200741\n",
      "Test set: Avg. loss: 2.1902, Accuracy: 223/1000 (22%)\n",
      "\n",
      "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 2.200150\n",
      "Test set: Avg. loss: 2.1851, Accuracy: 271/1000 (27%)\n",
      "\n",
      "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 2.153262\n",
      "Test set: Avg. loss: 2.1671, Accuracy: 224/1000 (22%)\n",
      "\n",
      "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 2.155079\n",
      "Test set: Avg. loss: 2.1570, Accuracy: 242/1000 (24%)\n",
      "\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 2.142054\n",
      "Test set: Avg. loss: 2.1346, Accuracy: 260/1000 (26%)\n",
      "\n",
      "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 2.117176\n",
      "Test set: Avg. loss: 2.1054, Accuracy: 276/1000 (27%)\n",
      "\n",
      "Train Epoch: 2 [35840/50000 (71%)]\tLoss: 2.079345\n",
      "Test set: Avg. loss: 2.1078, Accuracy: 270/1000 (27%)\n",
      "\n",
      "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 2.107643\n",
      "Test set: Avg. loss: 2.0816, Accuracy: 277/1000 (27%)\n",
      "\n",
      "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 2.082984\n",
      "Test set: Avg. loss: 2.0879, Accuracy: 263/1000 (26%)\n",
      "\n",
      "Test set: Avg. loss: 2.0752, Accuracy: 273/1000 (27%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 2.130058\n",
      "Test set: Avg. loss: 2.0784, Accuracy: 281/1000 (28%)\n",
      "\n",
      "Train Epoch: 3 [5120/50000 (10%)]\tLoss: 2.070910\n",
      "Test set: Avg. loss: 2.0513, Accuracy: 274/1000 (27%)\n",
      "\n",
      "Train Epoch: 3 [10240/50000 (20%)]\tLoss: 2.052846\n",
      "Test set: Avg. loss: 2.0379, Accuracy: 252/1000 (25%)\n",
      "\n",
      "Train Epoch: 3 [15360/50000 (31%)]\tLoss: 2.025628\n",
      "Test set: Avg. loss: 2.0123, Accuracy: 322/1000 (32%)\n",
      "\n",
      "Train Epoch: 3 [20480/50000 (41%)]\tLoss: 2.033277\n",
      "Test set: Avg. loss: 2.0182, Accuracy: 293/1000 (29%)\n",
      "\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.973432\n",
      "Test set: Avg. loss: 2.0259, Accuracy: 298/1000 (29%)\n",
      "\n",
      "Train Epoch: 3 [30720/50000 (61%)]\tLoss: 2.026082\n",
      "Test set: Avg. loss: 2.0336, Accuracy: 282/1000 (28%)\n",
      "\n",
      "Train Epoch: 3 [35840/50000 (71%)]\tLoss: 2.013653\n",
      "Test set: Avg. loss: 1.9959, Accuracy: 295/1000 (29%)\n",
      "\n",
      "Train Epoch: 3 [40960/50000 (82%)]\tLoss: 2.004685\n",
      "Test set: Avg. loss: 1.9775, Accuracy: 302/1000 (30%)\n",
      "\n",
      "Train Epoch: 3 [46080/50000 (92%)]\tLoss: 1.956068\n",
      "Test set: Avg. loss: 1.9786, Accuracy: 278/1000 (27%)\n",
      "\n",
      "Test set: Avg. loss: 1.9553, Accuracy: 289/1000 (28%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 2.069729\n",
      "Test set: Avg. loss: 1.9581, Accuracy: 304/1000 (30%)\n",
      "\n",
      "Train Epoch: 4 [5120/50000 (10%)]\tLoss: 1.954492\n",
      "Test set: Avg. loss: 1.9532, Accuracy: 303/1000 (30%)\n",
      "\n",
      "Train Epoch: 4 [10240/50000 (20%)]\tLoss: 1.909512\n",
      "Test set: Avg. loss: 1.9223, Accuracy: 322/1000 (32%)\n",
      "\n",
      "Train Epoch: 4 [15360/50000 (31%)]\tLoss: 1.921375\n",
      "Test set: Avg. loss: 1.9129, Accuracy: 348/1000 (34%)\n",
      "\n",
      "Train Epoch: 4 [20480/50000 (41%)]\tLoss: 1.923632\n",
      "Test set: Avg. loss: 1.9129, Accuracy: 348/1000 (34%)\n",
      "\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.914304\n",
      "Test set: Avg. loss: 1.9708, Accuracy: 300/1000 (30%)\n",
      "\n",
      "Train Epoch: 4 [30720/50000 (61%)]\tLoss: 1.946272\n",
      "Test set: Avg. loss: 1.9077, Accuracy: 332/1000 (33%)\n",
      "\n",
      "Train Epoch: 4 [35840/50000 (71%)]\tLoss: 1.931676\n",
      "Test set: Avg. loss: 1.9303, Accuracy: 296/1000 (29%)\n",
      "\n",
      "Train Epoch: 4 [40960/50000 (82%)]\tLoss: 1.928553\n",
      "Test set: Avg. loss: 1.9390, Accuracy: 297/1000 (29%)\n",
      "\n",
      "Train Epoch: 4 [46080/50000 (92%)]\tLoss: 1.924865\n",
      "Test set: Avg. loss: 1.8850, Accuracy: 332/1000 (33%)\n",
      "\n",
      "Test set: Avg. loss: 1.9255, Accuracy: 306/1000 (30%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.961207\n",
      "Test set: Avg. loss: 1.9075, Accuracy: 313/1000 (31%)\n",
      "\n",
      "Train Epoch: 5 [5120/50000 (10%)]\tLoss: 1.889528\n",
      "Test set: Avg. loss: 1.8719, Accuracy: 342/1000 (34%)\n",
      "\n",
      "Train Epoch: 5 [10240/50000 (20%)]\tLoss: 1.884834\n",
      "Test set: Avg. loss: 1.9143, Accuracy: 314/1000 (31%)\n",
      "\n",
      "Train Epoch: 5 [15360/50000 (31%)]\tLoss: 1.907332\n",
      "Test set: Avg. loss: 1.8913, Accuracy: 306/1000 (30%)\n",
      "\n",
      "Train Epoch: 5 [20480/50000 (41%)]\tLoss: 1.866294\n",
      "Test set: Avg. loss: 1.8931, Accuracy: 323/1000 (32%)\n",
      "\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.868037\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch, True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4FxFNbjHGVZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "final_train_counter = []\n",
    "final_train_losses = []\n",
    "final_test_counter = []\n",
    "final_test_losses = []\n",
    "\n",
    "for i in range(0, n_epochs):\n",
    "  final_train_counter.append(i)\n",
    "\"\"\"\n",
    "for i in range(0, len(avg_train_loss)-1, 30):\n",
    "    index = int(i/10)\n",
    "    final_train_counter.append(train_counter[i])\n",
    "    final_train_losses.append((train_losses[i] + train_losses[i+1]) / 2)\n",
    "    final_test_counter.append(train_counter[i])\n",
    "    final_test_losses.append((test_losses[i] + test_losses[i+1]) / 2)\n",
    "\n",
    "final_train_counter.append(train_counter[len(train_counter)-1]) \n",
    "final_train_losses.append(train_losses[len(train_counter)-1])\n",
    "final_test_counter.append(train_counter[len(train_counter)-1])\n",
    "final_test_losses.append(test_losses[len(train_counter)-1])    \n",
    "    \n",
    "plt.plot(len(avg_train_loss), avg_train_loss, color='blue')\n",
    "plt.scatter(avg_train_loss[-1], avg_train_loss[-1], color='blue')\n",
    "plt.plot(len(avg_test_losses), avg_test_losses, color='red')\n",
    "plt.scatter(avg_test_losses[-1], avg_test_losses[-1], color='red')\n",
    "\n",
    "# print(len(test_counter))\n",
    "# print(len(test_losses))\n",
    "\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')\n",
    "\"\"\"\n",
    "\n",
    "plt.plot(final_train_counter, avg_train_loss, color='blue')\n",
    "plt.scatter(final_train_counter[-1], avg_train_loss[-1], color='blue')\n",
    "plt.plot(final_train_counter, avg_test_loss, color='red')\n",
    "plt.scatter(final_train_counter[-1], avg_test_loss[-1], color='red')\n",
    "# plt.plot(final_train_counter, test_accuracy_list, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fU20QrRFnQEL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "last_acc = []\n",
    "last_counter = []\n",
    "index = 0\n",
    "for acc in range(0, len(test_accuracy_list)-1) :\n",
    "  last_acc.append(test_accuracy_list[acc])\n",
    "  last_counter.append(final_train_counter[index])\n",
    "  index = index + 1\n",
    "\n",
    "plt.plot(last_counter, last_acc, color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJa35oEyHGVf"
   },
   "outputs": [],
   "source": [
    "def makeCifar10Dec(param):\n",
    "    if param == 0:\n",
    "        return \"airplane\"\n",
    "    elif param == 1:\n",
    "        return \"automobile\"\n",
    "    elif param == 2:\n",
    "        return \"bird\"\n",
    "    elif param == 3:\n",
    "        return \"cat\"\n",
    "    elif param == 4:\n",
    "        return \"deer\"\n",
    "    elif param == 5:\n",
    "        return \"dog\"\n",
    "    elif param == 6:\n",
    "        return \"frog\"\n",
    "    elif param == 7:\n",
    "        return \"horse\"\n",
    "    elif param == 8:\n",
    "        return \"ship\"\n",
    "    elif param == 9:\n",
    "        return \"truck\"\n",
    "    \n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "for i in range(0, 10):\n",
    "    batch_idx, (example_data, example_targets) = next(examples)\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        output = network(example_data)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        for i in range(6):\n",
    "            plt.subplot(2,3,i+1)\n",
    "            plt.tight_layout()\n",
    "            plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "            plt.title(\"Prediction: {}\".format(\n",
    "                output.data.max(1, keepdim=True)[1][i].item()))\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Adlhca7pHGVo"
   },
   "outputs": [],
   "source": [
    "# Reloading a already saved model\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "continued_network = Net()\n",
    "continued_optimizer = optim.SGD(continued_network.parameters(), lr=learning_rate,\n",
    "                                momentum=momentum)\n",
    "\n",
    "network_state_dict = torch.load('./results/model.pth')\n",
    "continued_network.load_state_dict(network_state_dict)\n",
    "\n",
    "optimizer_state_dict = torch.load('./results/optimizer.pth')\n",
    "continued_optimizer.load_state_dict(optimizer_state_dict)\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "for i in range(0, 2):\n",
    "    batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = continued_network(example_data)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        for i in range(6):\n",
    "            plt.subplot(2,3,i+1)\n",
    "            plt.tight_layout()\n",
    "            plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "            plt.title(\"Prediction: {}\".format(\n",
    "                output.data.max(1, keepdim=True)[1][i].item()))\n",
    "            \n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLlTgHruHGVt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Averaged TABU NN Dropout.ipynb",
   "provenance": [
    {
     "file_id": "165_z7nSqC3c0e88BEkLaVk6VLbOzPnF-",
     "timestamp": 1554971127153
    },
    {
     "file_id": "1GUow23zovvlLuNkMUkHz4zJzfehTEQl9",
     "timestamp": 1554791975444
    },
    {
     "file_id": "1zhoOPcwneUN3foYHbrjQWttdHus2hocz",
     "timestamp": 1554750187224
    },
    {
     "file_id": "1vRXEE00eunJuYh-9toP6O6KlXIJHwTjG",
     "timestamp": 1554747644571
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
